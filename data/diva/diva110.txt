Power-Eﬃcient Design of an Embedded Flash
Memory Management System
Master thesis
JONAS BRUNLÖF
December 2009
Supervisors:
Magnus Persson, KTH
Barbro Claesson, ENEA
Detlef Scholle, ENEA
Examiner:
Martin Törngren, KTH
iii
.
Master thesis MMK2009:99 MDA356
Power-Eﬃcient Design of an Embedded Flash Memory
Management System
Jonas Brunlöf
Approved:
Examiner:
Supervisor:
2009-12-17
Martin Törngren
Magnus Persson
Employer:
Contact person:
ENEA AB
Detlef Scholle
.
Abstract
.
The report is the result of a master thesis at ENEA AB during the fall of 2009. It
aims to create a speciﬁcation of ﬂash memory management system which focuses
on power eﬃciency and low RAM usage for embedded systems, and to design and
implement a prototype of such a system to facilitate further development toward the
created speciﬁcation. The system used by ENEA today is a Flash Translation Layer
(FTL). It has a complex structure which prohibits modiﬁcations and customization,
therefore a new ﬂash memory management system needs to be developed.
The suggested solution uses a translation layer called Metadata FTL (MFTL), where
ﬁle system metadata and userdata are separated from each other in order to improve
performance. The partition holding userdata uses a block-level mapped translation
layer system called Fully Associative Sector Translation FTL. The other partition,
holding metadata, will instead use a page-level mapped translation layer system
which also separates often modiﬁed data from data modiﬁed seldom. The separation
of data with diﬀerent update frequencies is executed by a page allocation scheme
called Modiﬁcation Aware (MODA).
The result of this report is a speciﬁcation of the system described in the section
above and an implemented prototype which has all the basic features of an FTL.
The implemented design can successfully be used instead of the old FTL with a few
restrictions. It can handle normal ﬁle system commands and can manage reboots
without loss of information. However, the main goal of the implemented design
is still to act as a prototype to facilitate further development toward the design
explained in the speciﬁcation.
iv
.
Examensarbete MMK2009:99 MDA356
Energieﬀektiv design av ett inbyggt
ﬂashminneshanteringssystem
Jonas Brunlöf
Godkänt:
Examinator:
Handledare:
2009-12-17
Martin Törngren
Magnus Persson
Uppdragsgivare:
Kontaktperson:
ENEA AB
Detlef Scholle
.
Sammanfattning
.
Denna rapport är resultatet av ett examensarbete på ENEA AB under hösten 2009.
Målet med arbetet är att skapa en speciﬁkation över ett hanteringssystem för ﬂash-
minnen som fokuserar på energieﬀektivitet och lågt RAM utnyttjande för inbyggda
system, samt att designa och implementera prototyp som kan verka som grund
för att vidareutveckla systemet mot den framtagna speciﬁkationen. Det system som
idag används av ENEA är ett translationslager (FTL). Det har en komplex struktur
vilket förhindrar modiﬁeringar och anpassningar, därför ska ett nytt hanteringssys-
tem för ﬂashminnen tas fram.
Den framtagna lösningen använder ett translationslager kallat Metadata FTL (MFTL)
där metadata och användardata separeras från varandra för uppnå bättre prestan-
da. Partitionen som håller användardata använder ett blocknivå-mappat transla-
tionslager kallat Fully Assosiative Sector Translation FTL, vilket är designat för
minimera energikonsumtionen genom att begränsa kostsamma skriv- och raderop-
erationer till ﬂashminnet och samtidigt konsumera lite RAM. Den andra partitionen
som innehåller metadata använder istället ett sidnivå-mappat translationslager som
samtidigt separerar data som modiﬁeras ofta och data som sällan modiﬁeras för att
spara ännu ﬂer operationer. Separeringen av data med olika uppdateringsfrekvens
utförs av ett allokeringsschema som heter MODA.
Resultatet av denna rapport är en speciﬁkation över det system som är beskrivet
ovan samt en implementering av prototyp som har alla de grundläggande funktioner
ett FTL har. Den implementerade designen kan framgångsrikt användas istället för
det gamla FTL:et med några restriktioner. Det klarar normala ﬁlsystemkommandon
och kan hantera omstarter utan att tappa information. Fortfarande är det dock så
att den implementerade designen först och främst skall ses som en prototyp som
kan användas för vidareutveckling av systemet.
Contents
Contents
v
List of Figures
viii
List of Tables
x
Abbreviations
xi
1
Introduction
1
1.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Problem statement . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2.1
Flash memory management system . . . . . . . . . . . . . . .
1
1.3
Method
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.4
Delimitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2
Challenge: Merge energy eﬃciency and low RAM usage
5
2.1
Power management . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.1.1
Flash memory characteristics . . . . . . . . . . . . . . . . . .
6
2.1.2
RAM usage in embedded systems . . . . . . . . . . . . . . . .
7
2.2
Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
3
Flash memory management
9
3.1
An introduction to ﬂash memory . . . . . . . . . . . . . . . . . . . .
9
3.1.1
Functionality of a ﬂash memory . . . . . . . . . . . . . . . . .
9
3.1.2
Flash memory types . . . . . . . . . . . . . . . . . . . . . . .
10
3.1.3
Wear leveling . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
3.1.4
Garbage collection . . . . . . . . . . . . . . . . . . . . . . . .
12
3.1.5
Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
3.2
Overview of ﬂash memory management systems . . . . . . . . . . . .
13
3.3
Current setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
3.3.1
JEFF - Journaling Extensible File system Format
. . . . . .
14
3.3.2
Current FTL . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
3.4
Flash ﬁle systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
3.4.1
JFFS - Journaling Flash File System . . . . . . . . . . . . . .
14
v
vi
CONTENTS
3.4.2
YAFFS - Yet Another Flash File System . . . . . . . . . . . .
16
3.4.3
CFFS - Core Flash File System . . . . . . . . . . . . . . . . .
17
3.4.4
MODA - MODiﬁcation Aware . . . . . . . . . . . . . . . . . .
18
3.4.5
Summary and discussion . . . . . . . . . . . . . . . . . . . . .
18
3.5
Flash Translation Layers . . . . . . . . . . . . . . . . . . . . . . . . .
20
3.5.1
BAST FTL - Block-Associative Sector Translation FTL . . .
21
3.5.2
AFTL - Adaptive FTL . . . . . . . . . . . . . . . . . . . . . .
22
3.5.3
FAST FTL - Fully-Associative Sector Translation FTL . . . .
23
3.5.4
FTL/FC - FTL/Fast Cleaning
. . . . . . . . . . . . . . . . .
23
3.5.5
MFTL - Metadata FTL . . . . . . . . . . . . . . . . . . . . .
24
3.5.6
Summary and discussion . . . . . . . . . . . . . . . . . . . . .
24
3.6
Other ﬂash memory related functionalities . . . . . . . . . . . . . . .
26
3.6.1
Bad block management
. . . . . . . . . . . . . . . . . . . . .
26
3.6.2
ECC - Error correction code
. . . . . . . . . . . . . . . . . .
26
3.6.3
Cleaning policies . . . . . . . . . . . . . . . . . . . . . . . . .
26
3.6.4
Buﬀering
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.7
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.7.1
Optimal ﬂash management system . . . . . . . . . . . . . . .
27
3.7.2
Related questions and thoughts . . . . . . . . . . . . . . . . .
28
3.7.3
Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
4
OSE 5.4 and the Soft Kernel Board Support Package (SFK-BSP) 31
4.1
OSE 5.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
4.1.1
Processes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
4.1.2
Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
4.1.3
Flash Access Manager . . . . . . . . . . . . . . . . . . . . . .
33
4.2
Soft Kernel Board support package (SFK-BSP) . . . . . . . . . . . .
34
4.2.1
Modules in the soft kernel . . . . . . . . . . . . . . . . . . . .
34
5
Design and implementation
35
5.1
Translation layer design . . . . . . . . . . . . . . . . . . . . . . . . .
35
5.1.1
Initiation of translation layer . . . . . . . . . . . . . . . . . .
36
5.1.2
Storage on ﬂash volume . . . . . . . . . . . . . . . . . . . . .
36
5.1.3
Translation layer list . . . . . . . . . . . . . . . . . . . . . . .
36
5.1.4
Translation layer metadata
. . . . . . . . . . . . . . . . . . .
37
5.1.5
Scan function . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
5.1.6
Garbage collection . . . . . . . . . . . . . . . . . . . . . . . .
39
5.2
Implementation in OSE
. . . . . . . . . . . . . . . . . . . . . . . . .
39
5.2.1
Supported signals . . . . . . . . . . . . . . . . . . . . . . . . .
39
5.2.2
Supported I/O commands . . . . . . . . . . . . . . . . . . . .
40
5.2.3
Mount recommendations . . . . . . . . . . . . . . . . . . . . .
41
6
Test suite
43
6.1
Introduction to test
. . . . . . . . . . . . . . . . . . . . . . . . . . .
43
CONTENTS
vii
6.2
Test case 1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
6.2.1
Results from test case 1 . . . . . . . . . . . . . . . . . . . . .
44
6.3
Test case 2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
6.3.1
Results from test case 2 . . . . . . . . . . . . . . . . . . . . .
46
6.4
Test case 3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
6.4.1
Results from test case 3 . . . . . . . . . . . . . . . . . . . . .
48
6.5
Test case 4
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
6.5.1
Results from test case 4 . . . . . . . . . . . . . . . . . . . . .
50
6.6
Test case 5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
6.6.1
Results from test case 5 . . . . . . . . . . . . . . . . . . . . .
52
6.7
Summary of test results . . . . . . . . . . . . . . . . . . . . . . . . .
53
7
Discussion
55
7.1
Problem statement . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
7.1.1
Flash memory management design . . . . . . . . . . . . . . .
55
7.1.2
QoS and power awareness . . . . . . . . . . . . . . . . . . . .
56
7.1.3
File system and FTL interface
. . . . . . . . . . . . . . . . .
56
7.1.4
Flash memory management implementation . . . . . . . . . .
56
7.1.5
Evaluation of implementation . . . . . . . . . . . . . . . . . .
57
7.2
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
7.2.1
Evaluation of requirements
. . . . . . . . . . . . . . . . . . .
57
7.2.2
Summary of requirement evaluation
. . . . . . . . . . . . . .
58
8
Future work
61
8.1
Build according to speciﬁcation . . . . . . . . . . . . . . . . . . . . .
61
8.2
Dynamic support for diﬀerent ﬂash sizes . . . . . . . . . . . . . . . .
61
8.3
Flash translation layer metadata
. . . . . . . . . . . . . . . . . . . .
62
8.4
Test system on hardware . . . . . . . . . . . . . . . . . . . . . . . . .
62
Bibliography
63
A Complete test cases
65
A.1 Test case 1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
A.2 Test case 2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
A.3 Test case 3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
A.4 Test case 4a . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
A.5 Test case 4b . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
A.6 Test case 5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
B Requirements
71
List of Figures
3.1
NAND ﬂash architecture . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
3.2
Flash memory management system architecture . . . . . . . . . . . . . .
13
3.3
JFFS garbage collection . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3.4
The MODA scheme classiﬁcation . . . . . . . . . . . . . . . . . . . . . .
18
3.5
Page- and block-address translation
. . . . . . . . . . . . . . . . . . . .
21
3.6
Merge and switch operations
. . . . . . . . . . . . . . . . . . . . . . . .
22
3.7
DAC regions translation . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
4.1
Overview of process states . . . . . . . . . . . . . . . . . . . . . . . . . .
32
4.2
Structure of signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
5.1
Object in linked list
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
5.2
The structure of a metadata chunk . . . . . . . . . . . . . . . . . . . . .
37
5.3
Structure of delete-metadata chunk . . . . . . . . . . . . . . . . . . . . .
38
5.4
Overview of implementation in OSE . . . . . . . . . . . . . . . . . . . .
40
6.1
Example of a result printout
. . . . . . . . . . . . . . . . . . . . . . . .
43
6.2
Result from test case 1:a . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
6.3
Result from test case 1:b . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
6.4
Result from test case 1:c . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
6.5
Result from test case 2:a . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
6.6
Result from test case 2:b . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
6.7
Result from test case 3:a . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
6.8
Result from test case 3:b . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
6.9
Result from test case 3:c . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
6.10 Result from test case 3:d . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
6.11 Result from test case 3:e . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
6.12 Result from test case 3:f . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
6.13 Result from test case 3:g . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
6.14 Result from test case 4:a . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
6.15 Result from test case 4:b . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
6.16 Result from test case 4:c . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
6.17 Result from test case 4:d . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
6.18 Result from test case 5:a . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
viii
List of Figures
ix
6.19 Result from test case 5:b . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
A.1 Complete result from test case 1
. . . . . . . . . . . . . . . . . . . . . .
65
A.2 Complete result from test case 2
. . . . . . . . . . . . . . . . . . . . . .
66
A.3 Complete result from test case 3
. . . . . . . . . . . . . . . . . . . . . .
67
A.4 Complete result from test case 4a . . . . . . . . . . . . . . . . . . . . . .
68
A.5 Complete result from test case 4b . . . . . . . . . . . . . . . . . . . . . .
69
A.6 Complete result from test case 5
. . . . . . . . . . . . . . . . . . . . . .
70
List of Tables
2.1
NAND ﬂash characteristics
. . . . . . . . . . . . . . . . . . . . . . . . .
6
3.1
Overview of ﬂash ﬁle system properties
. . . . . . . . . . . . . . . . . .
19
3.2
Optimal ﬂash ﬁle system . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3.3
Evaluation summary of discussed translation layers . . . . . . . . . . . .
26
7.1
Evaluation of the requirements . . . . . . . . . . . . . . . . . . . . . . .
59
B.1
Descriptions of the requirements
. . . . . . . . . . . . . . . . . . . . . .
71
x
Abbreviations
JEFF
Journaling Extensible File system Format
JFFS
Journaling Flash File System
YAFFS
Yet Another Flash File system
CFFS
Core Flash File System
MODA
MODiﬁcation-Aware
FTL
Flash Translation Layer
AFTL
Adaptive Flash Translation Layer
BAST
Block-Associative Sector Translation
FAST
Fully-Associative Sector Translation
FTL/FC
FTL/Fast Cleaning
MDT
Memory Technology Device
DAC
Dynamic dAta Clustering
XIP
eXecute In Place
EEPROM
Electrically Erasable Programmable Read-Only Memory
LRU
Least Recently Used
FAM
Flash Access Manager
SFK-BSP
SoFt Kernel Board Support Package
xi
Chapter 1
Introduction
1.1
Background
The master thesis was done at ENEA AB in collaboration with the Royal Institute of
Technology, KTH. It is a part of GEODES, a project with the aim to provide power
awareness and innovative management capabilities for operating systems, protocols
and applications, and also to apply the notion of quality of service (QoS)[10]. Power
awareness can be considered as a concept within QoS and to implement it in QoS,
feedback from aﬀected devices is required. This master thesis will focus on creating
a speciﬁcation of an optimized ﬂash memory management system where QoS can
be implemented on a ﬂash media, and on designing and implementing a prototype
of a such a system.
1.2
Problem statement
In an embedded system it is important to minimize power consumption at every level
in the design. However, this needs to be achieved without reducing the performance
of the system in an unsatisfactory manner or disrupting any crash safety features.
Minimizing power consumption but still maintaining performance is an important
part of QoS and this can be helped by making modules power-aware.
1.2.1
Flash memory management system
A ﬂash memory management system involves many functions besides the obvious
read and write. Due to the construction and properties of ﬂash media, functions
such as erase, garbage collection, error correction and wear leveling are also needed.
ENEA’s own operating system OSE1 is using the JEFF2 ﬁle system and a ﬂash
translation layer (FTL) to handle ﬂash media. This complete ﬂash management
system needs to be optimized and previous studies at ENEA shows that the FTL
1Operating System Embedded
2Journaling Extensible File system Format
1
2
CHAPTER 1. INTRODUCTION
is the bottleneck. The FTL is a complete ﬂash media manager involving all the
functions stated above. However, it is a general purpose system and unnecessary
features are included in the software.
Flash memory management system design
Instead of analyzing and modifying the old FTL a new ﬂash media manager is
requested. Hence, other, preferably open source, solutions for ﬂash memory man-
agement require evaluation. What ﬂash management systems are available and is
there a solution ﬁtting the requirements? As a result, a speciﬁcation of how an op-
timized and power-aware ﬂash memory management system should be constructed,
will be acquired. How can power awareness and QoS be implemented on the ﬂash
memory management system?
In case of a need for an interface between a novel ﬁle system and the ﬂash
memory management system, optimization could be required. How can a potential
interface between a ﬁle systems and an optimized ﬂash memory management system
be improved?
Flash memory management system implementation
A prototype of a ﬂash memory management system is to be implemented in OSE but
due to time restrains it is not necessary that the implementation and the optimized
design are identical. What will a suitable implementation design look like to be able
to be constructed within the limited time frame?
When the design is implemented, veriﬁcation and validation of the design needs
to be performed to conﬁrm that the system can manage the basic requirements of a
ﬂash memory management system. Can the implemented design manage the basic
requirements of a ﬂash memory management system?
1.3
Method
The master thesis starts with an academic study involving research papers, manuals,
technical reports and books in the area of ﬂash memory management systems.
It also involves an evaluation of the diﬀerent ﬂash media management systems
found during the study. It ends in design speciﬁcation describing the requirements
obtained during the academic study.
The master thesis also includes an implementation, which is supposed to illus-
trate the ideas suggested in the design phase. A demonstration of the implemen-
tation and a report covering all the stages in this master thesis are also required.
1.4. DELIMITATIONS
3
1.4
Delimitations
The time limit for this master thesis is 20 weeks. The academic study and the
evaluation of the results need to be completed within the ﬁrst ten weeks.
The
design and implementation phases need to be completed within the remaining ten
weeks together with the rapport and the presentation.
The foundation for the speciﬁcation is only the data gathered during the aca-
demic study.
The implementation needs to end in a prototype with the basic features of a
ﬂash memory management system but not manage all the features fulﬁlled by the
speciﬁcation.
During development the OSE 5.4 operating system will be used together with a
software kernel from ENEA.
Chapter 2
Challenge: Merge energy eﬃciency and
low RAM usage
In an embedded system resources are limited and it is important to be restrictive
when using them. But the limited resources are even more important to consider
during the design of a system. For example, a goal to minimize power consumption
usually comes on the expense of something else, e.g. RAM-usage. Therefore, a
system needs to be able to ﬁnd a good compromise between declared criteria in
order to be considered optimized.
2.1
Power management
Power management involves monitoring the energy consumption and also changing
it to meet performance and energy demands. Power management is usually split in
two separate areas; static and dynamic power management.
Static methods of power management are predictions of how a system will func-
tion followed by design modiﬁcations to ﬁnd the best compromise between perfor-
mance and energy consumption. However, static power management is not always
a suﬃcient solution. In a system with a very dynamic workload the static power
management method sometimes needs to be adjusted to the worst case scenario,
and is therefore only optimal when the system is fully utilized.
Dynamic methods analyze the system during run-time and adjust the perfor-
mance to the present demand of resources. A branch in QoS focusing solely on
the energy aspect and on dynamic power management was developed by Pillai et
al. [17] and is called Energy-aware QoS (EQos). The Pillai et. al. EQoS method
includes making the best use of limited stored energy and also varying the service
level in each process to ensure that system runtime goals are met.
First of all it is preferred that a system saves as much energy as possible with-
out aﬀecting performance; this can be applied to both static and dynamic power
management. It is also applicable in both hardware and software implementations.
Dynamic voltage scaling (DVS) of the CPU or GPU is, for example, a well known
5
6
CHAPTER 2. CHALLENGE: MERGE ENERGY EFFICIENCY AND LOW RAM
USAGE
hardware approach for dynamic power management which can save energy without
aﬀecting performance [17]. In software eﬀective algorithms can be optimized for
minimizing energy consumption.
At a certain level it is no longer possible to save energy without aﬀecting the
performance of the system. The question then is what is of most importance; per-
formance or energy consumption. Sometimes it is necessary to degrade performance
because the energy source is depleting. However, the opposite is just as plausible; If
a very important task needs to be executed maximum performance can be requested
by the system.
2.1.1
Flash memory characteristics
Because of more energy eﬃcient processors, CPUs are no longer alone at the top
of energy consumers in embedded systems. Storage devices are becoming bigger
and faster and thus requiring more energy.
Flash memories in particular have
undergone a huge expansion in the last decade. Power-aware ﬂash memories with
dynamic power management properties are therefore requested.
Flash memory management systems have, however, special characteristics. Read,
write and erase operations have diﬀerent latency and requires diﬀerent amount of
energy [14].
Table 2.1 shows the latency and energy consumption of a typical NAND ﬂash
memory. It shows that write operations cost far more than read operations and
erase operations cost even more in both latency and energy. This aﬀects how power
management on ﬂash memories is carried out and prioritized. Not so much energy
can be saved if the number of read operations are minimized but reduction of write
and erase operations will show a great impact on energy consumption.
It is important to note that minimizing the number of read, write and ultimately
erase operations in this report not mean that the amount of read and write requests
to the ﬂash volume is to be aﬀected. The decrease of these operations instead aims
at the internal read, write and erase operations due to copying overhead during
ﬂash memory management operations, a more extensive explanation can be found
in Chapter 3.
Operation
Latency
Energy consumption
Page read
47.2 µs
679 nJ
Page write
533 µs
7.66 µJ
Block erase
3 ms
43.2 µJ
Table 2.1. NAND ﬂash characteristics [14]
2.2. REQUIREMENTS
7
2.1.2
RAM usage in embedded systems
The amount of RAM as primary storage in devices has been increasing steadily
since the day it was introduced and will probably continue to increase, but there is
still a need to be restrictive with RAM usage. In embedded system low RAM usage
is of extra importance due to low cost demands and space restrictions. The ﬂash
memory management system will therefore need to consider the amount of RAM
used.
2.2
Requirements
The following requirements for the ﬂash memory management system can be derived
from this chapter.
REQ1: The design of the system shall be power-eﬃcient.
REQ2: The system shall strive to be power-aware.
REQ3: The number of erase and write operations shall be minimized.
REQ4: The system shall use as little RAM as possible.
Power-eﬃciency has the highest priority of the four requirements stated above.
However, consideration still needs to be given to the other requirements to make
sure that none of them reach exaggerative proportions.
Chapter 3
Flash memory management
The ﬁrst of two main goals of the master thesis is to form a speciﬁcation of how an
optimized ﬂash memory management scheme should be constructed. This chapter
includes a brief introduction to ﬂash memory for readers with no prior knowledge in
the area, followed by an explanation of diﬀerent ﬂash memory management schemes
available and ends with a discussion of the presented schemes and a design descrip-
tion of the best solution.
3.1
An introduction to ﬂash memory
The ﬂash memory industry has exploded the last decade. Flash memory is now
one of the top choices when it comes to storage media in embedded systems. The
technology continues to improve and expand into new areas. The capacity nearly
doubles every year and solid state disks (SSD) are beginning to become a serious
competitor to regular hard drives [20].
3.1.1
Functionality of a ﬂash memory
A ﬂash memory is a non-volatile memory and a speciﬁc group in the EEPROM
family. Non-volatile memory has the advantage over volatile memory such as DRAM
and SRAM by being able to hold stored data without supply of power. Because
of a ﬂash memory’s low power consumption, shock resistance and small size, it
has many advantages compared to other non-volatile memory such as regular hard-
drives. Flash memory can also access data randomly, unlike hard-drives which suﬀer
from seek time because of their sequential data access properties [19, 4, 16].
Handling a ﬂash drive requires three diﬀerent main operators, read, write and
erase, in contrast to hard-drives which only require read and write.
The erase
operator is needed because overwrites is not possible on ﬂash. Instead of overwrites,
updated data is written to a new location in the ﬂash memory and the old data is
marked as invalid or dead. This is also known as out-of-place updates. Over time
the amount of invalid data in memory increases and will, if not handled correctly,
9
10
CHAPTER 3. FLASH MEMORY MANAGEMENT
ﬁll up the whole ﬂash memory. To reclaim an area occupied by invalid data the
erase operator is used. Once a memory area has been erased, it is free to be used
again by the system [4].
Flash memory also suﬀers from another disadvantage.
A ﬂash cell can only
handle a limited amount of erase cycles before it becomes unreliable or faulty. This
hardware problem can be addressed in software. The solution is to use the whole
memory area as evenly as possible and make sure that the diﬀerent sections of ﬂash
memory area are erased approximately the same number of times. This method
avoids a situation where one ﬂash area is worn out before all the others. This whole
process of evening out the load on the ﬂash memory is called wear leveling [19].
3.1.2
Flash memory types
There are typically two main types of ﬂash memory: NAND and NOR ﬂash. They
are both built with ﬂoating gate transistors but the two types diﬀer in the layout
of these transistors. This report will focus on NAND ﬂash because it is most wildly
used in embedded systems today [14, 4]. A brief explanation of the two will follow
but the rest of the report is based on the NAND ﬂash memory. Some of the content
can, however, be applied to both.
NOR ﬂash
The NOR ﬂash memory gets its name from the resemblance of a logical NOR gate.
The ﬂoating gates are connected in parallel just as a NOR gate which makes it
possible to access them individually.
This gives NOR ﬂash fast random access
speed but is instead more costly and is less dense in its architecture compared to
NAND. The possibility to access each cell individually makes NOR ﬂash ideal for
eXecution In Place (XIP) where programs can be executed directly on the ﬂash
without ﬁrst being copied to RAM.
Devices today usually have a small NOR ﬂash to boot from because of its short
read latency and its XIP abilities but use a NAND ﬂash memory for storing data
[4].
NAND ﬂash in general
The NAND ﬂash has the advantages of being smaller and cheaper to manufacture
compared to NOR, it also has faster write and erase cycles. The drawback with
NAND ﬂash is that it can only be accessed in sizes of a page where a page typically
contains 512 bytes. This makes NAND ﬂash unsuitable for XIP and more suited
for storage [4].
Figure 3.1 shows the architecture of a NAND ﬂash memory. The memory is
arranged in blocks with a typical size of 16 KB where each block consists of 32
pages. The read and write operators can act on page basis but erase operation can
only be done on a whole block [4].
3.1. AN INTRODUCTION TO FLASH MEMORY
11
This creates another diﬃculty when working with NAND ﬂash. If the block
that is to be erased still contains pages with valid data these have to be copied to
another block before the erase can take place. The copying of valid data and the
erase procedure is called garbage collection.
Also seen in Figure 3.1 is the layout of a page and its dedicated spare area. The
spare data area is reserved for ﬂash management metadata such as logical block
address and erase-count although it is up to the designer of the ﬂash management
system to use it as he see ﬁt. For pages with a size of 512 bytes the spare area will
be 16 bytes. The page and the spare area can be written/read independently but
as they exist in the same block they are removed together on erase [14].
Page 1
Page 2
Page 3
Page 4
Page 32
User data
Spare data
Flash pages in block
Flash blocks in 
memory
Figure 3.1. NAND ﬂash architecture
NAND ﬂash development
NAND ﬂash is the type of ﬂash memory which has been subject to most research
and development in recent years.
It is also the type mostly used in embedded
systems. A few years ago a new type of NAND ﬂash memory was developed which
changed the conditions for ﬂash management system completely. In older NAND
ﬂash memories it was possible to write to a page two or three times before an erase
needed to take place. It was also possible to write to arbitrary pages within a block
[15].
With the new NAND ﬂash hardware only one write is allowed before an erase
needs to take place and arbitrary writes is no longer possible. Instead pages in
a block needs to be written sequentially from the ﬁrst page to the last. During
the same time as the release of the new NAND ﬂash hardware a bigger block size
was also introduced [15]. 2048 bytes per block became the regular size for the new
12
CHAPTER 3. FLASH MEMORY MANAGEMENT
NAND ﬂash while older usually have a block size of 512 bytes, although both sizes
can appear in either NAND ﬂash hardware version.
3.1.3
Wear leveling
Due to the fact that the ﬂash blocks only can sustain a limited amount of erase
cycles, typically 105 for NOR and 106 for NAND [20], the problem with potentially
worn out blocks needs to be addressed. This is where wear leveling comes in. Wear
leveling is the technique used to distribute block erases evenly across the whole ﬂash
volume [4].
First of all the ﬂash memory utilization needs to be even, but that is not enough.
Some data might be static and never updated or deleted. The blocks containing
this type of information will always hold data which is valid thus will never be
erased. The wear leveling mechanism could instead move the static data to a block
which have had many erase cycles in order to even out the wear. The wear leveling
mechanism can be included in the software of the ﬂash memory management system
but there are many diﬀerent ways to deal with the problem [20].
3.1.4
Garbage collection
Garbage collection is the process of reclaiming invalidated memory. When data is
modiﬁed and updated the ﬁle system will allocate new pages for the updated ﬁle
and the old pages will be invalidated or marked as dead. Without garbage collection
the ﬂash memory would become full with invalid data and no more free space would
be available.
The garbage collector reclaims the invalidated pages and makes them free again
by erasing blocks containing dead pages. After an erase the block is free to be used
for new data. However, it not certain that all pages in a block are invalid when an
erase needs to take place. The valid data then ﬁrst needs to be copied to another
block with free space before the erase can be executed [19].
Garbage collection can be triggered either when the amount of free space has
reached a certain threshold value or it can run in background when the system is
idle. It is important to note that garbage collection needs to be requested before
the ﬂash memory is fully utilized. A full memory has no free blocks for copies of
potential valid data which means that the system would deadlock itself. Therefore,
a few free blocks are always left for garbage collection purposes.
The eﬀectiveness of a garbage collector is very much depending on how data is
allocated in the ﬂash memory. Diﬀerent methods of data allocation are discussed
further in the Sections 3.4 and 3.5.
3.1.5
Requirements
Because NAND ﬂash memory is mostly used in embedded systems today the design
of the ﬂash memory management system shall focus on NAND ﬂash.
It is also
important that the ﬂash memory management system can handle the requirements
3.2. OVERVIEW OF FLASH MEMORY MANAGEMENT SYSTEMS
13
that the new type of ﬂash memory introduces. These requirements are therefore
introduced.
REQ5: The design of the system shall focus on the NAND ﬂash memory type.
REQ6: The system shall write pages in a block sequentially starting from the ﬁrst
page.
REQ7: A page shall only be written once before it is erased.
3.2
Overview of ﬂash memory management systems
Basically there are two diﬀerent ways to access a ﬂash memory, see Figure 3.2. The
ﬁrst way is to use a traditional ﬁle system like ext3 or FAT on top of a ﬂash trans-
lation layer. The translation layer maps the logical addresses addressed by the ﬁle
system to the actual physical address on the ﬂash. Besides supplying the transla-
tion between logical to physical addresses the ﬂash translation layer also provides
garbage collection and wear-leveling. The translation layer can also emulate the
ﬂash memory as a block device so that traditional ﬁle systems such as FAT and
ext3 can work against ﬂash just as it was a normal hard drive [4].
The other option is to have a ﬁle system speciﬁcally developed for ﬂash; two
such examples are JFFS or YAFFS. With a ﬂash dedicated ﬁle system there is no
need for a logical to physical address translation table, instead the ﬁle system keeps
track of the locations of the pages belonging to each ﬁle. Garbage collection and
wear leveling is in this case a part of the ﬂash ﬁle system [4].
To be able to control a ﬂash memory a Memory Technology Device (MTD)
driver is needed. This layer supports the primitive ﬂash memory operations, such
as read, write and erase [20]. The MTD is located on top of the ﬂash drive and
interfaces with either the ﬂash translation layer or the ﬂash ﬁle system.
Virtual File System
File system (ext3 , FAT... etc)
Flash Translation Layer (FTL)
Memoy Technology Device (MTD) drivers
JFFS
YAFFS
Flash Memory
Figure 3.2. Flash memory management system architecture
14
CHAPTER 3. FLASH MEMORY MANAGEMENT
3.3
Current setup
As of today ENEA uses the JEFF ﬁle system on top of a ﬂash translation layer
when working with ﬂash memory.
3.3.1
JEFF - Journaling Extensible File system Format
JEFF is developed by ENEA and is the ﬁle system currently used in ENEA’s oper-
ating system OSE. JEFF is a journaling ﬁle system, i.e. changes to the ﬁle system
are ﬁrst written to a journal before changes are made to the actual data. This is
done to ensure the consistency of the ﬁle system in event of a crash. If a crash
occurs during a transaction and data is corrupted the system can restore itself to
the previous state by reading the journal. This means that transactions are either
performed completely or not at all, i.e. transactions are atomic in JEFF [7].
JEFF is designed to run on block devices but has the ability to adjust the block
size to suit the layer it is operating above; common block sizes are 512, 1024 and
2048 bytes.
3.3.2
Current FTL
The current FTL emulates itself as a block device and allows JEFF to access it
as if it was a hard-drive.
The FTL includes all the required features of a ﬂash
management system such as garbage collection and wear leveling [7]. However, it is
a general purpose system and unnecessary features are included in the software.
3.4
Flash ﬁle systems
This section will take a closer look at a few available ﬂash ﬁle systems and try to
pinpoint the important elements in them. It seems reasonable to start from the
very beginning.
3.4.1
JFFS - Journaling Flash File System
JFFS was the ﬁrst ﬁle system designed speciﬁcally for ﬂash media.
JFFS was
designed for NOR ﬂash and is a log-structured ﬁle system, this means that it writes
data sequentially on the ﬂash chip [18]. It was developed in 1999 by the Swedish
company Axis Communications and can be seen as the predecessor for all dedicated
ﬂash ﬁle systems of today.
The basic idea of JFFS is that it uses a circular log. It starts to write data in
the head of the log which at the ﬁrst entry will be at the beginning of the ﬂash area.
JFFS then continues to write data sequentially in the tail of the log and invalidates
old data on the way. This works ﬁne until the ﬂash runs out of free space and the
tail of the log is about to reach the end of the ﬂash.
Figure 3.3 shows what happens when free space in JFFS is running low and the
garbage collector starts to act. When the amount of free space reaches a certain
3.4. FLASH FILE SYSTEMS
15
level, Figure 3.3(a), JFFS garbage collection is initiated. It simply starts at the
head of the log and checks if the data is valid or invalid. Valid data is copied to
the tail and invalid data is ignored, Figure 3.3(b). This process is continued until
all valid data in an erase block has been copied to the tail. The erase block is then
erased and is clean and available to use for new data, Figure 3.3(c).
(a) Memory 
nearly full
(b) Copying 
valid data
(c) After garbage 
collection
Valid
Invalid
Free
Figure 3.3. JFFS garbage collection
The problem with this method is that the garbage collector works sequentially
and will clean blocks even if they only contain valid data. However, this method will
provide perfect wear leveling because every block is cleaned exactly the same amount
of times, but on the other hand are blocks cleaned when cleaning is unnecessary.
At mount time, the whole ﬂash memory is scanned and the location of all nodes1
in the memory are stored in RAM. File reads can then be performed immediately
without extra computation by looking at the data structures held in RAM and then
reading the corresponding location on the medium. The drawback of this method
is that it is very memory consuming.
The problem with garbage collection and some other issues made it obvious that
JFFS needed to be improved. The second version of the ﬂash ﬁle system is called
JFFS2. It was developed by David Woodhouse and is in many ways very similar to
JFFS. Two of the diﬀerences are that JFFS2 has limited support for NAND ﬂash
alongside with the NOR support and an improved garbage collector.
JFFS2 separates ﬁles in three diﬀerent lists. The clean list holds blocks with
valid data, the dirty list contains blocks with at least one obsolete page and the
free list contains erased blocks. Instead of selecting all the blocks sequentially as
in JFFS, the garbage collector in JFFS2 takes a block from the dirty list when
1Building blocks of the ﬁle system, both metadata and data are stored in nodes
16
CHAPTER 3. FLASH MEMORY MANAGEMENT
garbage collection is requested.
This method leads to a wear leveling problem.
Blocks containing static data are never updated and will never be erased whilst
other blocks will be erased constantly. This problem is addressed by letting the
garbage collector select a block from the clean list once every hundred time.
JFFS2 still scans the whole ﬂash memory at mount to index all the valid nodes.
Thus the RAM footprint and mount time increases linearly with the amount of data
stored on the ﬂash memory [16]. JFFS2 was ﬁrst designed for small ﬂash devices
and this issue becomes obvious with ﬂash sizes over 128 Mbytes [3].
Both JFFS and JFFS2 are released under the general purpose license (GPL).
3.4.2
YAFFS - Yet Another Flash File System
YAFFS was written by Aleph One speciﬁcally for NAND ﬂash ﬁle systems. It was
developed because it was concluded that JFFS and its successor was not suitable
for NAND devices [9].
YAFFS was the ﬁrst ﬂash ﬁle system to fully utilize the spare area in each page.
Just as JFFS it is a log-structured ﬁle system. By the time of developed the type
of ﬂash devices available supported writes to arbitrary pages within a block. It was
also possible to write to a page two or three times before an erase was needed. This
was used when a ﬁle had been updated and old pages needed to be invalidated. A
bit in the spare area of the aﬀected pages was rewritten and set to 0 to show that
they were invalid.
YAFFS uses a tree structure which provides the mechanism to ﬁnd all the pages
belonging to a particular ﬁle [15]. The tree holds nodes containing 2-byte pointers
to physical addresses. The 2-byte data is, however, not enough to individually map
each page on a larger ﬂash memory. For that reason, YAFFS uses approximate
pointers which instead point to a group of pages. The pages themselves are self
describing and that makes it possible to search each page in the group individually
to ﬁnd the right page. In this way the RAM-footprint is smaller compared to JFFS2
[9].
At mount only the spare area, containing the ﬁle ID and page number, of each
page needs to be scanned. The result is faster mount compared to JFFS2 but the
mount time will still increase linearly with the size of the ﬂash memory [16].
No particular wear leveling function is used in YAFFS. When blocks are allo-
cated for storage they are chosen sequentially so no block will repeatedly be left
unused. On the other hand, no consideration is taken to blocks which are allocated
with static data [1]. The author argues that wear leveling is not as important for
NAND ﬂash because the system needs to address bad blocks2 anyway. So even if
uneven wear will lead to a few more bad blocks the ﬁle system will still continue to
work properly [9].
Garbage collection in YAFFS has two modes: passive and aggressive. Passive
garbage collection only cleans blocks which have a big majority of invalidated data
2Bad blocks are described in Subsection 3.6.1
3.4. FLASH FILE SYSTEMS
17
and is active when there is a lot of free blocks available. The Aggressive garbage
collection is activated when the amount of free space starts to run out. It will then
clean more blocks even if there are many valid pages in them. [15]
A few years after YAFFS was introduced, the ﬂash memory hardware changed;
it was no longer possible to write to a page more than just once and writes to
pages within a block had to be made sequentially. This meant that the method of
invalidating pages was not allowed any more. This was why YAFFS2 was developed.
Instead of invalidating pages a sequence number was added in the spare area to make
it possible to determine which page that is still valid when a ﬁle was updated. Each
time a new block is allocated the sequence number is incremented and each page
in that block is marked with that number. The sequence shows the chronological
order of events, thus making it possible to restore the ﬁle system. [15]
Just as with JFFS both YAFFS and YAFFS2 are released under GPL and have
its source code available on the internet. YAFFS has been a very popular ﬁle system
among computer scientists and many new ﬁle systems are based on YAFFS. One of
them is the Core Flash File System (CFFS) explained in the next subsection.
3.4.3
CFFS - Core Flash File System
CFFS is based on YAFFS and the fundamental structure is the same but some
improvements have been made.
The blocks in CFFS have three classiﬁcations; inode-stored blocks, data blocks
and free blocks. The inode-stored blocks contain the locations of all data in the
memory.
This means that only the inode-stored blocks need to be scanned at
mount time. To be able to locate the inode-stored blocks at mount their locations
are written to an Inodemapblock at unmount and the Inodemapblock is always the
ﬁrst physical block in the ﬂash memory. The method of saving a snapshot of the
data structure on the ﬂash memory at unmount is usually called checkpointing [16].
The separation of inode-stored blocks and data blocks in CFFS has another
advantage besides the faster mount; it also improves the eﬀectiveness of the garbage
collector. Metadata is updated more often than regular data, e.g., renaming, moving
and changing attributes of a ﬁle will only change the metadata and not the regular
data. By separating metadata and data in diﬀerent blocks the probability that all
the pages in a block will be invalidated around the same time increases. This will
decrease the copying overhead in the garbage collector and in that way save both
energy and time. The separation of data according to their update frequencies is
called hot-cold separation or data clustering [16].
The separation between metadata blocks and data blocks, however, creates a
wear leveling issue. Because of the fact that metadata is updated more often, the
metadata blocks will be erased more frequently than the data blocks. This is solved
by using a weight value in each block. If the block was an inode-stored block last
time it will be allocated as a data block next time it is erased, thus solving that
wear leveling problem.
18
CHAPTER 3. FLASH MEMORY MANAGEMENT
3.4.4
MODA - MODiﬁcation Aware
The MODA scheme is not a complete ﬁle system; it is only a modiﬁcation in the page
allocation scheme in YAFFS. The MODA page allocator is a further development
of the one used in CFFS. It separates not only metadata and userdata, it also
distinguishes between diﬀerent update frequencies of userdata [2].
The MODA scheme uses a queue to classify how often a ﬁle is modiﬁed. The
ﬁle stays in the queue for a speciﬁc amount of time and its classiﬁcation depends
on how many times the ﬁle is modiﬁed during this period. Figure 3.4 shows an
overview of the separation.
When a page is allocated to a speciﬁc area it will stay there during its life
time even if its update frequency changes. The garbage collector mechanism used
in MODA operates in each area independently to avoid mixture of pages between
blocks with diﬀerent classiﬁcations [2].
Data
Meta data
User data
Hot-modified
user data
Cold-modified
user data
Unclassified
user data
Level0
Level1
Level2
Figure 3.4. The MODA scheme classiﬁcation
3.4.5
Summary and discussion
This subsection will serve as a summary of this section and will also include a
discussion of the six ﬂash management schemes presented above. Table 3.1 shows
an overview of the most signiﬁcant diﬀerences of the presented ﬂash management
schemes.
As a part of the discussion, this subsection will also reference to the requirements
established earlier in the report. A complete table of these requirements can be
found in Appendix B.
Individual ﬂash management scheme evaluation
According to REQ5, JFFS can be ruled out as an optimized solution because it
is developed for NOR ﬂash and not NAND ﬂash. JFFS2 has, on the other hand,
a limited support for NAND ﬂash but its functionality is surpassed by YAFFS.
However, JFFS2 is the only ﬁle system which has a wear leveling function which
handles uneven wear caused by static data.
Although YAFFS/YAFFS2 has no wear leveling function they are preferred in
front of JFFS2 because they have a better garbage collector, also used in CFFS and
3.4. FLASH FILE SYSTEMS
19
FFS
MT
GC policy
Wear leveling
Data clustering
JFFS
slower
Collects
each
block sequentially
regardless of the
contents
of
the
block
Not
needed
be-
cause
of
the
garbage
collec-
tor’s behavior
No clustering
JFFS2
slower
Selects
random
block
with
at
least one obsolete
page
Once every hun-
dred time a clean
block is chosen for
garbage collection
No clustering
YAFFS
YAFFS2
slow
Random selection
within the bound-
aries of the pas-
sive and aggres-
sive modes.
No wear leveling
No clustering
CFFS
fast
Same as YAFFS
Weight
value;
metadata
block
last time -> data
block next time
Separates
meta-
data
and
user
data
MODA
slow
Same as YAFFS
No wear leveling
Separates
meta-
data and userdata
and uses hot-cold
separation
of
userdata.
Table 3.1. Overview of the diﬀerent ﬂash ﬁle system properties.
FFS = Flash File system, MT = Mount time, GC = Garbage Collector
MODA, which considers the amount of valid data left in a block and not only that
the block has at least one obsolete page. Nevertheless, YAFFS needs to be ruled
out in favor of YAFFS2 because the older version fails to meet REQ6 and REQ7.
When the data clustering policy is considered there are only two options avail-
able; separation of metadata and userdata as in CFFS or as in MODA, where the
userdata is also separated in hot and cold areas.
When considering REQ1 and REQ3 it seems that the MODA allocation scheme
is the better solution because it is an improvement of the one used in CFFS. How-
ever, CFFS has the beneﬁt of using checkpointing and thus has the best mount time
of all schemes presented.
REQ2, which is the requirement concerning the introduction of power awareness,
has not been applied by any of the ﬂash management schemes and can therefore
not be used to facilitate the choice of the best scheme.
20
CHAPTER 3. FLASH MEMORY MANAGEMENT
Combination of the best ﬂash management scheme features
Although an optimized solution cannot be found when looking at these schemes
individually a combination of them can lead to a good result.
The foundation of the optimized solution will therefore be the CFFS scheme be-
cause of its fast mount properties. The data clustering scheme is, however, changed
to the MODA variant. The wear leveling scheme can also be combined with the
one used in JFFS2 to be able to handle wear cased by static data.
An optimized solution will, according the above stated arguments, look like
Table 3.2.
FFS
MT
GC policy
Wear leveling
Data clustering
Optimized
fast
Random selection
within the bound-
aries of the pas-
sive and aggres-
sive modes.
Metadata
block
last time -> data
block
next
time
and
once
every
hundred
time
a
clean block is cho-
sen
for
garbage
collection.
Separates
meta-
data and userdata
and uses hot-cold
separation
of
userdata.
Table 3.2. Optimal ﬂash ﬁle system.
FFS = Flash File system, MT = Mount time, GC = Garbage Collector
3.5
Flash Translation Layers
Just as there are many diﬀerent ﬂash memory ﬁle systems available there are also
many diﬀerent ﬂash translation layers (FTL). This report will explain and discuss
a few of the most signiﬁcant ones. As explained in Section 3.2 the main function of
the FTL is to translate the logical block addresses to a physical block addresses.
There are two major alternatives adopted for the translation table; page-level
mapping seen in Figure 3.5(a) and block-level mapping depicted in Figure 3.5(b)[11,
19]. The page-level mapping translation scheme maps each logical sector number
to each physical page number. The mapping table will therefore have one entry for
each page on the ﬂash memory. The block-level mapped translation scheme splits
the logical sector number into a logical block number and a page oﬀset instead. The
data stored in the mapping table for the block-mapping technique is only the logical
to physical block numbers. This means that the block-level mapping approach needs
extra operations to translate the logical block numbers and page oﬀset to a physical
address but consequently it consumes far less RAM [4, 11].
Being restrictive with RAM usage is very important especially in products de-
veloped for mass production and choosing the right mapping table can make a huge
diﬀerence. For example a 4 Gbyte NAND ﬂash with the large block size of 2048
3.5. FLASH TRANSLATION LAYERS
21
Kbyte requires 8 Mbytes of RAM for maintaining the page-level mapping table
while the block-level mapping table only requires 128 Kbyte [11]. For this reason,
varieties of the block-level mapping translation schemes are mostly used today.
Logical 
sector 
number
Page-level
mapping table
Physical 
page 
number
Flash block
Physical block number
Logical 
sector 
number
Block-level
mapping table
Physical block number
Locical page number
Flash block
(a) Page-mapped FTL
(b) Block-mapped FTL
Logical 
block 
number
Figure 3.5. Page- and block-address translation
Most of the resent ﬂash translation layers have variations of a scheme using log
blocks. They usually have an overall block-mapping scheme but introduces page-
level management for a few blocks [12]. A few schemes using log blocks and one
using another method are explained further in the following subsections.
3.5.1
BAST FTL - Block-Associative Sector Translation FTL
The Block-Associative Sector Translation (BAST) scheme is a translation layer
developed by Kim et al. It manages the majority of the blocks at block-level but
a number of blocks are managed at the ﬁner page-level. The former blocks are
referred to as data blocks and hold ordinary data, the latter are called log blocks
and are used for temporary storage for small writes to data blocks [12].
When a page in a data block is updated, a log block is allocated from a pool
with available free blocks. Because of the out-of-place update characteristics of a
ﬂash medium the update is written to the log block instead where the writes are
performed incrementally from the ﬁrst page and onward. A log block is dedicated
to a speciﬁc data block and if a page in another block needs to be updated a new
log block is allocated. The updates can be carried out until the log block is full,
when this happens a merge operation takes place.
In the merge operation, see Figure 3.6(a), a new free block is allocated and the
valid data is copied from the data block and the log block to the free block. Note
that the internal page locations are kept intact so that the page oﬀset in the block-
level mapping does not need to change. The free block becomes the new data block
and the other two blocks can be erased [12].
During special circumstances the merge operation can be replaced with a switch
operation, see Figure 3.6(b). This happens when all the pages are updated sequen-
tially. No new free block is then required, the log block can instead directly be
turned into a data block and the old data block can be erased. This is an ideal
22
CHAPTER 3. FLASH MEMORY MANAGEMENT
situation and saves a lot of energy because no copying overhead is needed and one
erase operation is saved [12].
1
2
3
4
5
6
1
5
5
2
2
2
Data block
Free block
Log block
Free block
Free block
Data block
(a) Merge 
(b) Switch
1
2
3
4
5
6
Data block
Free block
1
2
3
4
5
6
Log block
Data block
1
2
3
4
5
6
Data block
Free block
1
2
3
4
5
6
Log block
Data block
Figure 3.6. Merge and switch operations
During both the merge and switch operation the data is moved to another
block, thus, the mapping information needs to be updated. According to Kim et al.
previous schemes have had reverse physical to logical mapping information stored
in the spare area of each page. This requires scanning of the whole ﬂash at mount
time to be able to locate all mapping information. Instead Kim et al. purposes a
mapping table stored in dedicated blocks called map blocks to enable faster mount.
At mount only the map blocks are scanned and a map over the map blocks, the
map directory, is stored in RAM. When a page is updated both the corresponding
map block and the map directory needs to be updated.
However, this ensures
the consistency of mapping table even at an unexpected power failure and thus
simplifying the recovery.
3.5.2
AFTL - Adaptive FTL
Another variant to the log block scheme called Adaptive FTL is purposed by Wu
and Kou. AFTL uses a combination of a block-level mapping scheme and a page-
level mapping scheme. Two hash tables are held in RAM, one for the each mapping
table but the page-level mapped table has a limited amount of slots available. Wu
and Kou uses a log block when pages are updated but instead of using the merge
operation, described in Figure 3.6(a), when the log block is full AFTL leaves the
log block intact and stores its valid data in the page-level hash table. The argument
is that this data can be considered as hot data and is more likely to be accessed
frequently thus requires a page-level mapping [19].
There are only a ﬁnite number of page-level mapping slots available and which
pages that staying in the hash are handled by a linked list using the Least Recently
Used (LRU) policy [19]. When the list is full a new entry will force old pages to be
but back in the block-level mapped hash table. The pages are then returned to a
block and inserted in their original position to make sure that the page oﬀset still
is pointing to the right data.
3.5. FLASH TRANSLATION LAYERS
23
3.5.3
FAST FTL - Fully-Associative Sector Translation FTL
The Fully-Associative Sector Translation (FAST) scheme is developed by Lee et al.
and it is built on the BAST scheme, but it introduces two important diﬀerences.
The ﬁrst is that FAST adopts fully-associative address-mapping.
The second is
that FAST uses two diﬀerent kinds of log blocks; one kind for sequential writes and
another for random writes [13].
Fully-associative address-mapping means that a log block no longer is associ-
ated with a particular data block. Instead updates from many data blocks can be
stored in the same log block. This method actually introduces the need for the
second diﬀerence. As shown in Figure 3.6 a switch operation is superior to a merge
operation because it needs no copying overhead and only one erase operation. But
with the fully-associative method its highly unlikely that a merge operation ever
occurs. This is where the sequential log block make its contribution.
When a write is taking place the system ﬁrst checks if the page which is being
updated is the ﬁrst page in a data block i.e. logicalsectornumber mod numberofpa-
gesinablock = 0. If it is the ﬁrst page it is put ﬁrst in the sequential writes (SW)
log block. The data (if any) already in the SW log block is merged with its data
block before the insertion. If the following updates are coming sequentially they
will continue to ﬁll the SW log block and when it is full a switch operation can take
place. However, if the data is not written sequentially the data is added to the SW
log block anyway but only a merge operation can be applied when it is full or when
another ﬁrst block needs to be inserted.
The random write (RW) log blocks are used when a sequence of writes does
not start with a page from the ﬁrst position in a block. A switch operation can
never occur for a RW log block, only merge operations. The merge for RW log
blocks is, however, a bit diﬀerent from the merge shown in Figure 3.6(a). It still
involves one log block but can involve many data blocks due to the fully-associative
address-mapping. A comparison between BAST and FAST has been done by Lee
et al. and it shows that FAST can reduce the erase count by 10-50% depending on
test case and is in a worst case at the same level as BAST [13].
3.5.4
FTL/FC - FTL/Fast Cleaning
FTL/Fast Cleaning (FTL/FC) is a translation layer developed to speed up cleaning
time for larger ﬂash memories. FTL/FC is not a log block based translation layer,
instead it uses a data placement policy called Dynamic dAta Clustering (DAC).
DAC is a ﬂash memory management scheme for logical partitioning of storage
space. The idea is to cluster data with similar update frequencies together. Fig-
ure 3.7 shows how DAC operates. Data which is updated frequently will be moved
upwards towards the top region and be considered as hot data, not so frequently
updated data will instead end up in the bottom regions as cold data [4].
A new page of data is ﬁrst written to the bottom block. A promotion to an
upper region can only happen if the page is updated and it is not older than a
24
CHAPTER 3. FLASH MEMORY MANAGEMENT
predeﬁned “young-time”. If the page is updated after the “young-time” deadline it
will stay in the same region. Demotions to a lower level happen when blocks are
selected for cleaning. Pages in the selected block that are still valid and older than
a predeﬁned “old-time” will be demoted to the previous level, younger pages are
written back to the current region [5].
Region 2
...
Region n
Top
Region 1
Bottom
Too old
Too old
Too old
Too old
Updated 
& young
Updated 
& young
Updated 
& young
Updated 
& young
Figure 3.7. DAC regions translation
Each region includes multiple LRU lists; there is one list for every number of
invalid pages a block can have, i.e., if the block layout is 32 pages there will be 32
LRU lists in each region. There will also be a separate cleaning list shared with all
regions holding the blocks with no valid data [4].
In FTL/FC DAC is set to partition the memory in three diﬀerent regions, hot,
neutral and cold.
The cleaning policy used in FTL/FC makes use of the multiple LRU lists in
each region. First of all the blocks in the cleaning list are selected for cleaning. If
the cleaning list is empty the cost beneﬁt policy, see Subsection 3.6.3 for details,
is used. Instead of having to search through all the blocks to ﬁnd the optimal one
for cleaning only the ﬁrst block in each LRU list needs to be searched. The other
blocks can be ignored because the cost-beneﬁt policy wants the oldest block and
that will always be the ﬁrst block in the list [4].
3.5.5
MFTL - Metadata FTL
Wu et al. purposes a ﬁle system aware FTL, named MFTL, with the ability to
separate metadata and userdata. Wu et al. argues that metadata is accessed more
often than userdata and metadata also usually consist of very small ﬁles. Small
ﬁles does not use a considerable amount of memory and therefore can a page-
level mapping scheme be used for the metadata partition without too much RAM
overhead. The userdata, however, can still be handled on block-level [20].
Writes in the page-level mapped area are performed sequentially in a logging-
fashion.
3.5.6
Summary and discussion
This subsection will summarize the current section and also discuss the beneﬁts
and drawbacks of the ﬁve translation layers presented with the goal of choosing
an optimized solution.
References to the requirements will be made during the
discussion, a table with all requirements can be found in Appendix B.
3.5. FLASH TRANSLATION LAYERS
25
Individual evaluation
In the AFTL scheme the switching operations between the hash tables creates copy-
ing overhead. Performance gains like faster access to more frequently accessed data,
however, makes up for this overhead. The problem with AFTL is that when pages
are switched from the page-level to the block-level mapped hash table writes are
performed to a speciﬁc page in the designated block. This is required to make sure
that the page oﬀset still points to the right data but it also means that AFTL does
not meet the sequential write demand of REQ6.
Another consideration that needs to be taken into account is what is most impor-
tant; energy saving or memory consumption. The only FTL of the ones discussed
here able to handle hot and cold separation is FTL/FC using the DAC technique.
However, the DAC technique does not keep inter-block locations of pages intact.
Therefore, FTL/FC cannot use a block-level mapping scheme and have to use the
much more memory-consuming page-level mapping scheme instead, and thus going
against REQ4. This might be acceptable in systems with a lot of physical memory
but for embedded systems where RAM usage is crucial it is not a suitable solution.
When considering the two translation schemes BAST and FAST, the FAST
translation layer is, as mentioned above, an improvement of the BAST scheme and
is therefore a more suitable choice.
The last translation layer, MFTL, is not really a competitor to the others, it
is more a compliment. Separation between metadata and user data can be imple-
mented in any of the other FTL:s. Here ENEA has an advantage compared to the
developers of MFTL, ENEA also controls the ﬁle system JEFF. The ﬁltering tech-
nique used in MFTL is therefore not even necessary. It is already possible for JEFF
to inform the translation layer whether the sent data is metadata or userdata.
When REQ2, with its strive towards power-awareness, is considered it can be
concluded that none of the described translation layers apply any functionalities to
support power-awareness.
Optimized solution
The optimized solution would, because of the above stated arguments, be the FAST
FTL on the block-level mapped userdata in MFTL and make use of JEFF so that
the translation layer is aware of if incoming data is userdata or metadata.
This solution will meet requirements; REQ1, REQ3, REQ4, REQ5, REQ6 and
REQ7. REQ2 can also be considered to be met because a strive towards power
awareness has taken place although it could not be applied in any of the presented
translation layers.
The discussion in Subsection 3.5.6 is summarized in Table 3.3.
26
CHAPTER 3. FLASH MEMORY MANAGEMENT
FTL
Opinion derived from discussion
AFTL
Cannot handle new ﬂash memories with sequential write requirement
FTL/FC
Uses too much RAM for use in embedded systems
BAST
Surpassed by FAST
FAST
Optimal ﬂash transition layer for block-level mapping
MFTL
Preferable solution in collaboration with JEFF and FAST
Table 3.3. Evaluation summary of discussed translation layers
3.6
Other ﬂash memory related functionalities
This section describes functionalities which are embedded in a ﬂash memory man-
agement scheme but are not fully elaborated in this study.
3.6.1
Bad block management
NAND ﬂash are designed to get high density for a low cost and a perfect ﬂash
memory is not guaranteed in production. Usually new ﬂash memory has a few bad
blocks which are unusable and a few more are expected to go bad during its lifetime
[15]. Bad block management is a feature which makes bad blocks invisible to the
system. It can be done either in hardware or software but every good ﬂash memory
management system needs a bad block manager.
3.6.2
ECC - Error correction code
Just as NAND ﬂash requires bad block management, it also needs error correction
code to handle frequent bit errors.
ECC does not need to ask a sender if the
received message was correct to detect an error; it is capable of detecting a speciﬁc
amount of errors within a certain quantity of data by itself. The ECC feature can
be implemented in hardware or a separate software application or in the ﬁle systems
and translation layers themselves.
3.6.3
Cleaning policies
This subsection describes diﬀerent cleaning policies used by a garbage collector in
either a ﬂash ﬁle system or a ﬂash translation layer.
One of the simplest cleaning policies is the greedy policy; it always selects the
block with the most amount of invalid data. The goal is to reclaim as much free
space as possible in each garbage collection [4]. The greedy policy has been proven
to be eﬃcient when data is accessed uniformly. However, if some data is updated
more frequently than other, also known as hot data, it would be preferred that this
data is not copied because it will soon be invalidated anyway. The greedy policy
does not consider this and can therefore not avoid copying of hot data [14].
3.7. DISCUSSION
27
Another cleaning policy called cost-beneﬁt policy does, however, consider hot
data [4, 14]. It calculates a value of each block on the ﬂash memory according to
the speciﬁc formula
benefit
cost
= age(1 −u)
2u
and the block which gets the highest value is selected for cleaning. The u is the
percentage of valid data in the selected block, hence (1−u) is the percentage of free
space that could be reclaimed. 2u simulates the cleaning cost; one u for reading the
valid data and one u for writing it to a free block. The age stand for the amount
of time elapsed from the latest update of the block. The age parameter increases
if blocks have not been reclaimed for a long time and the chances for that block to
be chosen for cleaning increases. In this way blocks which contain invalid data can
be cleaned more evenly than with the greedy policy [4].
Another policy which is very similar to the cost-beneﬁt policy is the CAT clean-
ing policy. It selects block for cleaning according to the minimal value from the
formula
u
1 −u
1
agenumcl
.
u and age are the same as for the Cost-beneﬁt policy. numcl is the number
of times the block has been cleaned.
u
1−u stands for the cleaning cost. The only
diﬀerence from the cost-beneﬁt policy is that CAT also considers the number of
times the block has been cleaned before.
3.6.4
Buﬀering
A write buﬀer in a ﬂash memory management system can reduce write requests to
the memory and thus save energy. Less write operation will also mean less erase
operations. Li et al. suggests that putting a buﬀer between the logical block address
layer and the physical block address layer can save energy be reducing writes to ﬂash
memory [14]. Buﬀer management is, however, out of the scope of this study.
3.7
Discussion
This section will discuss the beneﬁts and drawbacks of ﬂash ﬁle systems versus ﬂash
translation layers and present an optimal ﬂash memory management system.
3.7.1
Optimal ﬂash management system
Woodhouse [18] argues that in order to have a crash safe system journaling needs
to be integrated in both the ﬁle system and the translation layer. That is not an
eﬀective solution and therefore, Woodhouse suggests that a dedicated ﬁle system is
used instead.
28
CHAPTER 3. FLASH MEMORY MANAGEMENT
ENEA has, on the other hand, the well working ﬁle system JEFF and wants
to continue to use it. Another aspect is that ENEA’s operating system does not
always operate with ﬂash memories; it might as well work with hard drives and
a dedicated ﬂash ﬁle system is therefore not always useful. The translation layer
option is, because of the above stated arguments, a more suitable solution for ENEA
although it might not be an optimal solution if only performance is considered.
The optimized ﬂash memory management system purposed by this study is
the translation layer selected in Subsection 3.5.6.
It was the MFTL combined
with FAST FTL supported by the JEFF interface which already has the ability to
distinguish metadata from userdata.
3.7.2
Related questions and thoughts
It might be possible to combine the suggested optimal solution with the knowledge
drawn from Section 3.4. The parts concerning only ﬁle systems are not of interest
here but some solutions can be applied to both ﬂash dedicated ﬁle systems and
translation layers.
Data clustering with the DAC technique was dismissed because it required
page-mapping and thus too much RAM. The MODA scheme presented in Sub-
section 3.4.4, however, only separates data in the ﬁrst stage and will therefore not
disrupt the internal page locations during operation. It would therefore be possible
to combine the MODA scheme with the selected optimal translation layer. Meta-
data and userdata separation is already included in MFTL so the diﬀerence with
MODA would be the allocation of userdata in hot and cold regions.
On the other hand, it is not certain that any beneﬁts can be drawn from the
MODA scheme when the management system is using the block-level mapping table
for userdata and thus locking pages in their positions within a block.
The real
question is if any copying and erase operations are saved during merge and switches
be separating hot and cold data. This question is not answered in this study but it
could be interesting to look closer at.
Another question concerning the ideal situation of a switch operation is also
worth some thought. Is it possible to increase the chances of a switch operation
taking place? One solution could be to introduce a buﬀer between the logical block
address layer and the physical block address layer and then buﬀer pages until a
complete block can be written sequentially. In a case where the sequential log block
in nearly full but the last pages fail to come another solution could be to copy the
last pages from the data block directly to the sequential log block and then perform
the switch operation.
Wear leveling is an aspect not taken into account in the translation layer schemes.
It could be forth while to use some knowledge of wear leveling drawn from Section 3.4
and apply it to the selected solution.
Finally some thought should be put into what garbage collection policy to use
in the page-level mapped section in MFTL. Of the three policies mentioned in Sec-
tion 3.6.3 the CAT cleaning policy is probably the best choice. Using this policy will
3.7. DISCUSSION
29
also automatically introduce some wear leveling to the page-level section because of
its considerations to both the age parameter and the erase count.
3.7.3
Requirements
When looking at the discussion above at least one thing is clear. The selected choice
of an optimal ﬂash memory management system will use a translation layer together
with ENEA:s own ﬁle system JEFF.
REQ8: The ﬂash memory management system shall use a translation layer to-
gether with JEFF.
Chapter 4
OSE 5.4 and the Soft Kernel Board
Support Package (SFK-BSP)
4.1
OSE 5.4
OSE is an embedded real time operating system based on a microkernel architecture.
It is designed to manage both hard and soft real-time constraints. The operating
systems fundamental building blocks are processes, a process corresponds to threads
and tasks in other operating systems.
There are many ways for OSE processes to communicate, but the simplest and
most preferred way is by signals. Signals are typed messages sent from one process
to another.
This section will discuss the parts of OSE relevant to this master thesis.
4.1.1
Processes
The building blocks in OSE are the processes. This is because the systems use
processes to allocate CPU time for diﬀerent jobs. A process can be compared to
tasks or threads in other programming environments [6].
Only one process at a time can be executed on the CPU. To make sure critical
processes are executed ﬁrst, processes in OSE are assigned to diﬀerent priority levels
depending on how imperative they are for the system performance. If a process
with a high priority is ready to run but a process with lower priority is currently
executing on the processor, a context switch takes place.
The lower prioritized
process is preempted and moved to the ready state, while the higher prioritized
process moved to the running state and executed in the processor.
A process will always be in one of these three states:
Running The process is currently being executed on the processor.
Ready The process is ready to execute, but a process with higher priority is cur-
rently being executing on the processor.
31
32
CHAPTER 4. OSE 5.4 AND THE SOFT KERNEL BOARD SUPPORT PACKAGE
(SFK-BSP)
Waiting The process is waiting for an event to occur and is currently not in the
need of the CPU.
A process can move between these three states during system operation as par-
tially explained above. A more extensive explanation of these movements is given
in Figure 4.1.
Ready
Waiting
Running
Preemption
Dispatch
Start*
Stop
Start**
Receive
Figure 4.1.
Overview of the three process states and their rotation.
* Start of
process with lower or the same priority as the one running. ** Start of process with
higher priority than the one running.
4.1.2
Signals
The recommended tool for communication between processes in OSE is signals [6].
A signal is a message buﬀer that is sent from one process to another.
The signals contains a signal number, hidden signal attributes, data if necessary
and an end mark, see Figure 4.2.
A
B
End
Mark
Sig.
no
Adm.
block
Data field (optional)
Figure 4.2. A signal is sent from process A to B containing three or four blocks: An
administrative block (Adm.block), a signal number (Sig.no), a data ﬁeld (optional)
and an end mark (0xEE).
Adm. block The hidden attributes can be found in the Administrative block. The
administrative data is accessed by system calls and contain information about
which process that sent the signal, which process it was sent to, signal size,
etc.
4.1. OSE 5.4
33
Sig. no The signal number is an identiﬁcation number telling the receiving process
what kind of information to expect in the signal. Each signal number must
be assigned to a data structure before it can be used.
Data ﬁeld The data ﬁeld is optional and its existence and size is deﬁned in the
assigned data structure.
End mark The end mark marks the end of the signal with the value 0xEE.
4.1.3
Flash Access Manager
The Flash Access Manager (FAM) is a manager designed to give applications access
to a ﬂash memory. It is designed for NOR-ﬂash and can therefore not handle the
tougher requirements of NAND ﬂash memory. However, it is possible to manually
apply the restrictions of NAND ﬂash when using the FAM interface and, by this,
simulate that the FAM is controlling a NAND ﬂash device.
An application using the FAM communicates with it through the ﬂash_api.h
interface. The following functions are available to the application:
ﬂash_read: Function to read data from ﬂash.
ﬂash_write: Function to write data to ﬂash.
ﬂash_erase: Function to erase data from ﬂash.
get_ﬂash_characteristics: Gets start address, address range and erase-block
size of the ﬂash volume.
Applications using the FAM need to, in addition to the regular start address and
address range, be aware of the erase-block size to be able to perform erases. This
information can be extracted from the get_ﬂash_characteristics function together
with information about the address range of the volume and the start address [8].
The use of ﬂash_read, ﬂash_write and ﬂash_erase functions are simple. A start
address of the operation needs to be speciﬁed together with the number of bytes the
operation is to be performed on. The read and write functions also need a pointer
to a read and write buﬀer respectively [8].
Some restrictions of the addressing does, however, apply. The FAM itself can
read and write any number of bytes to an arbitrary address, if the FAM is used to
simulate control of a NAND ﬂash, reads and writes has to be page aligned and have
the size of a page. The same issue is applied to the erase operation but this time the
block size is the common denominator. The block align and erase size restriction
apply to both NOR and NAND ﬂash [8].
The FAM is not an OSE process. It operates as a BIOS module and inherits its
process priority from the calling client process. Calls to the FAM will, because of
the process inheritance, block all operations at lower priorities. This is especially
important to consider for the ﬂash_write and ﬂash_erase operation which take a
34
CHAPTER 4. OSE 5.4 AND THE SOFT KERNEL BOARD SUPPORT PACKAGE
(SFK-BSP)
very long time execute. As a result processes with lower priority are likely to be
starved during execution of these operations. Therefore it is important to ensure
that the process priority is low enough so that the systems real-time performance
is not destroyed [8].
4.2
Soft Kernel Board support package (SFK-BSP)
The development platform used in this master thesis has been the OSE soft kernel.
The soft kernel allows OSE processes to be run on a host computer; in this case
PC using the Windows operating system. The soft kernel also sets up a simulated
RAM and NOR-ﬂash memory on the host which are interpreted as real hardware
by the system [8]
The size of both the simulated RAM and the NOR-ﬂash can be set manually.
The simulated memories are built at the ﬁrst startup and stored as .mem ﬁles in
the root directory on the host hard drive, this is called a cold start. At reboot, only
a warm start is needed, which means that the .mem ﬁles are just read into the host
RAM.
4.2.1
Modules in the soft kernel
Auxiliary functions can be added to OSE by including modules in the compilation.
The included modules will in this way be contained in the created binary ﬁle.
The translation layer is constructed as such a module and is included in the
OSE build during compilation.
Chapter 5
Design and implementation
The second goal of this thesis is to design and implement a ﬂash memory manage-
ment system for the OSE operating system. The goal is not to build a complete
system but rather to build a foundation to facilitate future work. The ultimate goal,
in a longer perspective, is to design and implement the ﬂash memory management
system described in Section 3.7.
5.1
Translation layer design
This report focuses on the NAND ﬂash type for the reasons mentioned in Sec-
tion 3.1.1, however, suitable NAND ﬂash developing tools were not available. The
soft kernel version of OSE described in Section 4.1 includes a simulated environ-
ment for NOR-ﬂash. It was decided that the NOR-ﬂash simulator should be used
due to the absence of a ﬁtting NAND ﬂash simulator and the advantage of using
a simulator instead of working with actual hardware from the start. However, the
design should still consider the tougher requirements of NAND ﬂash. The following
requirements needed to be taken under consideration:
• Access to ﬂash memory can only be done in the size of a page. (REQ5)
• Writes needs to be made sequentially within a block. (REQ6)
• A page can only be written once before it is erased. (REQ7)
The translation layer is designed to be as simple as possible but still include
all the basic features required by a translation layer. Because of the demand for
simplicity a page-level mapped translation layer approach was chosen. This means
that the developed translation layer will consume more RAM than the proposed
optimal solution but will, on the other hand, be possible to implement within the
time constraints.
35
36
CHAPTER 5. DESIGN AND IMPLEMENTATION
5.1.1
Initiation of translation layer
At startup, before JEFF tries to mount the ﬂash volume, the translation layer
is registered in the kernel. Then the volume is scanned and the translation list
containing the active ﬁles is built, details about the scan function and the translation
list can be found in section 5.1.3 and Section 5.1.5 respectively.
After the scan the translation layer module waits for request signals from JEFF.
For instance, JEFF needs to know the size of the volume, which block size to use
and which signals the translation layer module can receive and process. When the
initiation process is ﬁnalized the ﬂash memory is mounted by the ﬁle system and is
then ready to be used for ﬁle storage.
5.1.2
Storage on ﬂash volume
The two ﬁrst blocks on the ﬂash memory are dedicated to translation layer meta-
data, for more information about this type of metadata see Section 5.1.4, the rest is
available for ﬁle system data. The data on the ﬂash memory is stored in a logging
fashion from the beginning of the address range to the end. When the end is reached
the translation layer starts to write in the beginning of the memory again. Garbage
collection makes sure that the area selected for writing is erased and free, for more
information see Section 5.1.6.
5.1.3
Translation layer list
The map storing the logical to physical address information is built as a linked list
and is stored in the system RAM. The linked list holds all data that is valid on
the memory at a given moment. The objects in the linked list holds the logical
address, the physical address and pointers to the previous and next object in the
list. Figure 5.1 shows an example of an object in the translation list.
Figure 5.1. Object in linked list
The translation list can handle four operations: add, search, remove and print.
The add operation is used when new data is stored on the ﬂash memory. A new entry
is then added in the end of the linked list holding the logical address and the actual
physical address. The search operation is used when the system needs to know if
a speciﬁc data object can be found on the memory or if a speciﬁc ﬂash address is
occupied by valid data. The remove operation is used when data is invalidated on
the ﬂash memory, note that the remove function will not do anything to the physical
5.1. TRANSLATION LAYER DESIGN
37
memory, it will only remove the entry from the list if found. The last operation,
print, is used if the user wants a printout of the logical to physical mapping of the
current valid data.
5.1.4
Translation layer metadata
The ﬁrst two blocks on the ﬂash memory are dedicated to translation layer meta-
data. Note that the translation layer metadata is not the same as the ﬁle system
metadata mentioned earlier. The ﬁle system, JEFF in this case, will send packages
containing either userdata or ﬁle system metadata, the translation layer metadata,
on the other hand, is created by the translation layer itself and is not received from
the ﬁle system.
The translation layer metadata holds the same information as the translation
list and is needed because the translation list cannot be maintained during a restart.
The translation list is stored in RAM and is therefore lost when the power is turned
oﬀ. It can only be recreated at restart if the data is also stored on the ﬂash memory.
The logical to physical data mapping is stored in a string of characters on the
memory. Again, this is far from an optimal solution because it requires a lot more
memory than necessary but it is simple and works suﬃciently well. The metadata
string structure can be seen in Figure 5.2.
Figure 5.2. The structure of a metadata chunk
PA stands for physical address and the following ten characters holds the actual
physical address, LA stands for logical address and the characters following holds
the logical address. The NULL character is added after the logical address to mark
the end of the string.
This data is written to one of the two blocks dedicated for metadata whenever a
change in the ﬁle system occurs. As with the ﬁle system data, the metadata is also
written in a logging fashion starting from the beginning of the block and continuing
to the end. When the end is reached the writes do not just continue to the next
block. Instead the information in the translation list is ﬁrst written to the second
block. The translation list holds all the live pages on the memory and is therefore
the only information that needs to be saved. When the translation list has been
written, the ﬁrst block can be erased because all valid information is now stored in
the second block. New updates are thereafter written to the second block until it
is full and the procedure starts over again.
If new data is added, a new metadata write is performed and a string with
the format shown in Figure 5.2 is written in the active metadata block. However,
updates and erases also needs to be written to the metadata block because they do
also make changes to the ﬁle system. Updates are simple, they are written in the
exact same way as a new added page. Erases needs to be handled separately, in this
38
CHAPTER 5. DESIGN AND IMPLEMENTATION
system erases are written on the format seen in Figure 5.3. The fact that metadata
writes are written sequentially makes it possible to determine which metadata entry
that is the most recent.
Figure 5.3. The structure of a metadata chunk which is to be deleted
Because of the fact that only one metadata block can be used at the same time
the number of pages possible to store on the ﬂash memory are limited. With the
current setup the live page limit is 2048.
The method used for writing the metadata is not portable to a NAND ﬂash
environment. This is because the current system writes the metadata in sizes of a
metadata chunk, which is 32 byte. NAND ﬂash can only write in sizes of a ﬂash
page with a size of 512 byte or 2048 byte.
From the beginning the idea was to fully simulate a NAND ﬂash memory and
also make use of the spare area described in Section 3.1.2. However, the method
of storing metadata with the format depicted in Figure 5.2 and Figure 5.3 was
already established and unfortunately not compatible with the spare area size. The
metadata chunks, with their size of 32 bytes, where to big to ﬁt in to the 16 bytes
of available space in the spare area.
The design of the metadata then had to
be redesigned to be able to ﬁt into the spare area, but because the focus of the
implementation was a solution as simple as possible it was not found necessary to
change the metadata write implementation. However, it needs to be change in the
future to accommodate the requirements of NAND ﬂash memory.
5.1.5
Scan function
The scan function is used when the translation layer is started and before the
ﬂash volume is mounted by JEFF. The scan function searches trough the metadata
stored in one of the two metadata blocks at the beginning of the ﬂash memory.
As mentioned in Section 5.1.4 the metadata chunks are formatted according to
Figure 5.2 and have the size of 32 bytes to be aligned with the block size. The
scan function looks at the start of a metadata chunk and conﬁrms that the ﬁrst two
characters are "PA". Then the function stores the following characters which are
the physical address, when "LA" is reached it breaks and instead stores the logical
address in another variable until the NULL character is found.
The scan starts at the beginning of the active metadata block and adds new
pages to the translation list when they occur. If a page that is already in the list
is encountered the physical address can just be updated. This eﬀect is due to the
certainty that the encountered object is a more recent update because metadata
updates are written sequentially in the metadata block. If a chunk with the deleted
format occurs, its object in the translation list is deleted on the same basis.
5.2. IMPLEMENTATION IN OSE
39
The scan continues until the beginning of a metadata chunk no longer holds any
data. All valid data is then back in the translation list and the ﬁle system can then
mount the volume and start to operate on it again.
5.1.6
Garbage collection
The garbage collection algorithm used in the translation layer is constructed in the
same way as the one used in JFFS, described in Section 3.4.1. The data is kept in an
ongoing log with a beginning and an end, and the free space is located either before
or after the data. When data needs to be erased, the ﬁrst block in the log of data is
chosen. The garbage collection algorithm will then go through the translation list
checking if any of the pages belong to the block selected for erasure. When a page
is found to belong to the selected block the data is read from the ﬂash memory and
then written in the end of the log. At the same time the physical location of the
page is updated in the translation list and a new metadata chunk holding the new
location is written to the metadata block.
When the whole translation list is scanned and all valid data is copied from the
erase block to the end of the log, the erase can take place.
The system is set to perform garbage collection when the number of free blocks
goes under a threshold value. It is, however, possible to perform garbage collection
manually if necessary.
5.2
Implementation in OSE
The new translation layer is implemented as a module between JEFF and the FAM
replacing the current FTL. Figure 5.4 shows the hierarchy of the processes handling
the ﬁle system and the interfaces through which the translation layer communicates.
The ﬁle system uses the ddb.sig interface when sending and receiving signals to
and from the translation layer which in turn communicates with the FAM module
through ﬂash_api.h.
5.2.1
Supported signals
The ddb.sig interface includes all necessary signals for a ﬁle system communicating
with a secondary storage volume. The translation layer can, however, not handle all
of them. The signals supported by the developed translation layer are the following:
DDB_EXAMINE_DISK_REQUEST Part of the initiation process. Requests
information about disk size, block size, cache size and if the device is readable,
writable and/or able to handle random access.
DDB_INTERFACE_REQUEST Part of the initiation process. Requests in-
formation about which signals the module can handle, the reply includes all
signals in this list.
40
CHAPTER 5. DESIGN AND IMPLEMENTATION
JEFF
Implemented FTL
Old FTL
FAM
Simulated flash
ddb.sig
flash_api.h
Figure 5.4. Overview of implementation of the new translation layer in the OSE
ﬁle system
DDB_SUPPORTED_IO_REQUEST Part of the initiation process. Requests
information about which commands in the DDB_IO_REQUEST that are
supported by the module.
DDB_MOUNT_REQUEST Part of the initiation process. Requests to mount
an address range on the ﬂash volume.
DDB_UNMOUNT_REQUEST A request to unmount an address range on
the ﬂash volume.
DDB_IO_REQUEST The signal used to request I/O operations.
5.2.2
Supported I/O commands
When the module is up and running all communication between JEFF and the
translation layer are managed through the DDB_IO_REQUEST signal. During
initiation JEFF asks which commands in this signal that are supported with the
DDB_SUPPORTED_IO_REQUEST. The translation layer supports the following
DDB_IO_REQUEST commands:
DDB_OPC_READ: Read request to a sequence of blocks
DDB_OPC_WRITE: Write request to a sequence of blocks
DDB_OPC_JUNK: Delete request to a sequence of blocks
DDB_OPC_WRITE_BARRIER: Requests write barrier, i.e. ensure that all
previous read, write and junk requests are made persistent on the media before
any following such requests are made persistent.
The implementation of these commands is fairly straight forward. The read/write/junk
requests hold a logical address, the number of pages in sequence to be aﬀected by
5.2. IMPLEMENTATION IN OSE
41
the command and a pointer to a read/write buﬀer. When the logical address is
translated to a physical address by the translation list the operations can be car-
ried out via the FAM interface. The last command, write barrier, is ignored by the
translation layer although it is supported. This is because the translation layer does
not use a cache and will therefore always make all read, write and junk requests
persistent on the media before any other requests are carried out.
5.2.3
Mount recommendations
The translation layer does only support access for one application with one partition
on the ﬂash volume. It is therefore unnecessary to mount anything but the whole
volume, because it would only result in unused space.
If more than one application needs to access the ﬂash modiﬁcations needs to be
made to the translation layer.
Chapter 6
Test suite
6.1
Introduction to test
This test suite will focus on the functionality of translation layer and the tests will
clarify if the implemented design meets the basic requirements of a translation layer.
The results will be printouts from the terminal and pieces of these printouts
are added as ﬁgures in the report to facilitate the understanding of the tests. The
complete printouts from all test cases can be found in Appendix A.
Figure 6.1 shows a short example of such a printout. The ﬁrst line shows that a
write operation containing userdata with the logical address 1123 has been written
to the physical address 0x30022C00. The second line shows that 29 Junk operations
will take place on logical addresses 1124 through 1152.
Other operations appearing in the tests are read operations, where data from a
physical address is read into RAM, and Update operations where an already existing
logical address is written to a new physical location.
Figure 6.1. Example of a result printout
6.2
Test case 1
Description: Test case 1 will test write, read, append and overwrite operations on
a ﬁle with a size smaller than a page (<512 byte).
Preparation: No preparation needed but if the exact same results depicted in the
test are required, the ﬂash needs to be restarted and formatted before the
start of the test.
Steps: The following steps are manually executed once in chronological order.
43
44
CHAPTER 6. TEST SUITE
1. Open test ﬁle tf1 (Parameter: w+)1
2. Write data to ﬁle
3. Read data from ﬁle
4. Close test ﬁle
5. Print content in ﬁle
6. Open test ﬁle (Parameter: a+)2
7. Append data to ﬁle
8. Read data from ﬁle
9. Close test ﬁle
10. Print content in ﬁle
11. Open test ﬁle (Parameter: w+)
12. Write data to ﬁle
13. Read data from ﬁle
14. Close test ﬁle
15. Print content in ﬁle
6.2.1
Results from test case 1
A complete printout of test case 1 can be found in Appendix A.1.
The ﬁrst part of the test is depicted in Figure 6.2. The test is initiated on line
1. Line 3-4 contain two metadata read requests from JEFF irrelevant to this test.
The creation of the ﬁle in step 1 does not produce any ﬂash operations.
1.
2.
3.
4.
5.
6.
7.
7.
8.
9.
Figure 6.2. Result from test case 1:a
Line 5 is a result of JEFF’s journaling process where upcoming changes to the
ﬁle system are written to a journal before any changes are made persistent on the
volume. On line 6 the write operation in step 2 is performed and the logical address
1120 is written to the physical address 0x30021600. In the next operation 31 pages
1Open empty ﬁle for reading and writing, if ﬁle already exists contents is erased and ﬁle is
treated as new empty ﬁle
2Open ﬁle for reading and appending
6.2. TEST CASE 1
45
are deleted stating from the logical address 1121. This is not part of the test case
but an automatic junk request from JEFF. The result of step 3 is not visible, this
is because the last used page is kept in RAM by JEFF and a read from ﬂash is
therefore not necessary. Step 4 does not generate any requests to ﬂash but when
step 5 is executed a printout occurs which can be seen on lines 8 and 9.
The test continues in Figure 6.3. Step 7, consisting of an append operation,
starts with a read at 0x30021600 on line 11, where the userdata of the ﬁle tf1 is
stored. The read operation is followed by an update to the new physical address
0x30021800 on line 12. Step 8 and 9 are again invisible but the result of step 10 can
be read on lines 13 through 15. Note that new data is appended to the old data
and the append operation has thereby been carried out correctly.
7.
8.
9.
10.
11.
12.
13.
14.
14.
15.
16.
17.
18.
19.
20.
21.
Figure 6.3. Result from test case 1:b
The last part of the test can be seen in Figure 6.4. At step 11 the junk operation
on line 16 is occurring due to the parameter in the open request where the old ﬁle
is deleted before the creation of a new with the same name. The metadata write
on line 17 refers to the journaling operation prior to the userdata write in the next
step. Step 12 is done at line 18 where new data is written to the new physical
address 0x30021C00. Step 13 and 14 are not visible but the result of the last step
can be read on the lines 20 to 22. The previous data is now erased and the ﬁle
only consists of new data, the overwrite operation can therefore also be declared
successful.
14.
15.
16.
17.
18.
19.
20.
21.
21.
22.
23.
Figure 6.4. Result from test case 1:c
46
CHAPTER 6. TEST SUITE
6.3
Test case 2
Description: Test case 2 will test write and read operations with a size bigger
than a page (>512 byte) and simultaneously make a stress test with many
consecutive read and write operations.
Preparation: No preparation needed, however, if the exact same results as the
results depicted in the test are required, test case 1 needs to be executed prior
to this test.
Steps: Step 1 to 4 are contained in a loop which iterate 512 times, the last step is
executed once.
1. Open test ﬁle tf2 (Parameter: w+)
2. Write 1242 bytes of data to ﬁle
3. Read 1242 bytes of data from ﬁle
4. Close test ﬁle
5. Print content in ﬁle
6.3.1
Results from test case 2
The complete printout of test case 2 can be found in Appendix A.2.
The ﬁrst part of the test is displayed in Figure 6.5. The test is initiated on line 1.
The creation of tf2 in step 1 does not result in any ﬂash operations. The metadata
write illustrated on line 4 is part of the journaling process. The write operation in
step 2 requires three pages to ﬁt all 1242 bytes and will therefore need three ﬂash
writes, all seen on line 5,6 and 7. Step 3 is executed on lines 9 and 10, the last page
is held in RAM by JEFF and is not needed to be read from ﬂash. Step 4 does not
cause any eﬀect on the ﬂash memory.
1.
2.
3.
4.
5.
6.
7.
7.
8.
9.
10.
11.
12.
13.
14.
Figure 6.5. Result from test case 2:a
These four steps are repeated 512 times but only iteration 1,2,511 and 512 is
visible in the ﬁgure in Appendix A.2. The junk operations on lines 12, 22 and 31
are the result of step 1 where the previous ﬁle is erased before it is recreated.
6.4. TEST CASE 3
47
The 512 iterations are executed without error and the last iteration together
with the ﬁrst three lines of the result of step 5, where the content in tf2 is printed,
can be seen in Figure 6.6.
28.
29.
30.
31.
32.
33.
34.
35.
35.
36.
37.
38.
39.
40.
41.
42.
42.
43.
44.
45.
46.
47.
48.
49.
Figure 6.6. Result from test case 2:b
The printout on lines line 39 to 60 in Appendix A.2 is a correct reproduction of
the written data and the stress test and the write and read operations with a size
bigger than a page can be considered successful.
6.4
Test case 3
Description: Test case 3 will test various ﬁle system commands and their eﬀect
on the ﬂash memory.
Requirements Test case 1 and 2 needs to be executed before this test case in
order to retrieve any useful information from it.
Steps: The following steps are manually executed once in chronological order.
1. Change to ﬂash directory (command: cd)
2. List ﬁles in directory (command: ls)
3. Copy content of test ﬁle tf1 to standard out (command: cat)
4. Copy test ﬁle tf1 to tf1_copy (command: cp)
5. Copy content of new ﬁle tf1_copy to standard out (command: cat)
6. Create test3 directory (command: mkdir)
7. Move test ﬁle tf2 to test3 directory (command: mv)
8. List ﬁles in ﬂash directory (command: ls)
9. Change to test3 directory (command: cd)
10. List ﬁles in test3 directory (command: ls)
48
CHAPTER 6. TEST SUITE
6.4.1
Results from test case 3
This test is not executed automatically, instead all steps are done manually from
the terminal. The complete printout of test case 3 can be found in Appendix A.3.
The results of step 1 and 2 are depicted in Figure 6.7. The directory change in
step 1 is made on line 1 followed by the listing of the ﬁles in the current directory
in step 2. Lines 3 to 5 shows the result of the list command where the two ﬁles
created in test case 1 and 2 are visible.
1.
2.
3.
4.
5.
6.
7.
Figure 6.7. Result from test case 3:a
Step 3 is initiated on line 6, seen in Figure 6.8, and results in a read at the
physical location of tf1 on line 8 and a printout of its content on line 8 and 9. The
printout shows the most up to date content of tf1 and is therefore regarded as a
success.
1.
2.
3.
4.
5.
6.
7.
7.
8.
9.
10.
11.
12.
13.
14.
Figure 6.8. Result from test case 3:b
Step 4 is commenced at the end of line 9 and results in another read at the
physical location of tf1 on line 10, see Figure 6.9. This operation is followed by
a metadata update which is a journaling operation prior to the userdata write on
line 12, where the copied data with logical address 1124 is written to the new
physical address 0x30124200. The write is followed by three junk operations and
four updates of metadata initiated by JEFF on lines 13 through 19.
7.
8.
9.
10.
11.
12.
13.
14.
14.
15.
16.
17.
18.
19.
20.
21.
Figure 6.9. Result from test case 3:c
Step 5, executed on line 20, shows that the copied data in tf1_copy holds the
same information as the original ﬁle, , see Figure 6.10. The copy operation is thereby
6.4. TEST CASE 3
49
considered a success.
14.
15.
16.
17.
18.
19.
20.
21.
21.
21.
22.
23.
24.
25.
26.
27.
Figure 6.10. Result from test case 3:d
The creation of a directory, test3, in step 6 is commenced on the end of line 22
in Figure 6.11. It involves metadata writes to the journal before and updates of the
ﬁle system metadata which can be seen on lines 23 through 30.
21.
21.
22.
23.
24.
25.
26.
27.
27.
28.
29.
30.
31.
32.
33.
34.
Figure 6.11. Result from test case 3:e
In step 7 tf2 is moved to the test3 directory which also only requires metadata
updates for the journal and the ﬁle system. See lines 32 through 36 in Figure 6.12
27.
28.
29.
30.
31.
32.
33.
34.
34.
35.
36.
37.
38.
39.
40.
41.
Figure 6.12. Result from test case 3:f
Step 8 lists the current ﬁles in the ﬂash directory on lines 38 to 41 in Figure 6.13
and it shows that the creation of the test3 directory was successful and the existence.
Step 9 followed by step 10 on line 42 and 43 respectively shows that the move
command also was accomplished without error.
34.
35.
36.
37.
38.
39.
40.
41.
41.
42.
43.
44.
45.
46.
Figure 6.13. Result from test case 3:g
50
CHAPTER 6. TEST SUITE
6.5
Test case 4
Description: This test case will show the translation layer list and the garbage
collection mechanism.
Preparation: Test case 1, 2 and 3 needs to be executed prior to this test.
Steps: The following steps are executed once in chronological order.
1. Print translation layer list (command: trans_print)
2. Print ﬂash usage (command: fusage)
3. Perform garbage collect on 14 blocks (command: garbage_collect)
4. Print translation layer list (command: trans_print)
5. Print ﬂash usage (command: fusage)
6.5.1
Results from test case 4
A complete ﬁgure of test case 4 is depicted in Appendix A.4 and Appendix A.5.
Figure 6.14 shows a compressed view of the translation layer list after test case
1, 2 and 3. The ﬁrst step, printing the translation layer, is executed on line 1. The
list contains 22 entries with mostly metadata. Userdata can only be found in ﬁve
entries; list entry 6 containing the logical address 1120 which represent the data in
ﬁle tf1, list entry 17-19 holding the logical addresses 1121-1123 containing the three
page long tf2 and list entry 20, which holds the copy of tf1, tf1_copy.
1.
2.
3.
4.
5.
...
18.
18.
19.
20.
21.
22.
23.
24.
25.
25.
18.
19.
20.
21.
21.
22.
23.
Figure 6.14. Result from test case 4:a
The translation layer list also supplies some additional information. On line 24
the value of the metadata counter is displayed. It shows how many of the 2048
available metadata slots that are currently in use in the current metadata block.
Line 25 also prints information about the number of free blocks available and the
start and end address of the data on the ﬂash medium.
Step 2 is initiated on line 27, visible in Figure 6.15. The fusage command is an
existing function communicating with the FAM-interface. Line 28 shows the address
range of the ﬂash memory, in this case 0x30000000-0x303FFFFF. The following data
6.5. TEST CASE 4
51
visualizes the memory usage of the ﬂash volume. The 16 lines with 4 columns (only
six are visible in Figure 6.15, the rest can be seen in Appendix A.5), from line 29
to 44 represent the 64 ﬂash blocks building the memory. The ﬁrst number is the
start address of the block in hex followed by the size of the block, in this case 64
K3. The next letter tells whether the block is used (U) or free (F).
21.
21.
22.
23.
24.
25.
26.
27.
27.
28.
29.
30.
31.
32.
33.
34.
34.
35.
36.
37.
38.
39.
40.
41.
Figure 6.15. Result from test case 4:b
The ﬁrst two blocks on line 29 are, as mentioned in Section 5.1.4, dedicated to
translation layer metadata. The ﬁrst block is used at the moment and the other is
free but a change will occur when the metadata counter reaches 2048. The rest of
the ﬂash memory is available for storage. So far block 3 through 19 are used by the
ﬁle system which correlate to the data_start and data_end information on line 25
in Figure 6.14.
The following steps are visible in Figure 6.16 which also is the ﬁrst part of
the ﬁgure in Appendix A.5. Normally the memory usage would continue until the
free block threshold is reached before garbage collection is commenced. However,
garbage collection can be forced, and that is done in step 3 on line 1.
1.
2.
3.
4.
5.
6.
7.
7.
8.
9.
10.
11.
12.
13.
14.
Figure 6.16. Result from test case 4:c
Garbage collection on 14 blocks is initiated and all valid data in those 14 blocks
is copied to the end. As seen on lines 3 to 6 only four pages are copied. If the ﬁgure
in Appendix A.4 is studied more closely it becomes clear that those four pages
are the only entries in the translation layer list with a physical address within the
aﬀected 14 blocks.
Additionally, line 7 shows that the metadata counter has increased because of
the four copy operations and now shows 1842.
As a result of step 4 the translation layer list after the garbage collect operation
can be seen in Appendix A.5 where the physical location of list entry 6 and 17-20
has been updated.
3JEDEC memory standard, K = 1024
52
CHAPTER 6. TEST SUITE
Figure 6.17 shows the relevant result from step 5, which is the ﬂash usage print-
out, where the number of used blocks has decreased to three. Line 33 also shows
that the number of free blocks has increased together with the updated new start
and end addresses of the data.
27.
28.
29.
30.
31.
32.
33.
34.
34.
35.
36.
37.
38.
39.
40.
41.
41.
42.
43.
44.
45.
46.
47.
48.
Figure 6.17. Result from test case 4:d
6.6
Test case 5
Description: This test case will test the ﬂash scan function used at reboot.
Preparation: No preparation needed but if the exact same results as the results
depicted in the test are requested test case 1, 2 3 and 4 needs to be executed
prior to this test.
Steps: The following three steps are executed once.
1. Exit soft kernel (command: sfkexit)
2. Restart soft kernel (command: source run)
3. Print translation layer list (command: trans_print)
6.6.1
Results from test case 5
The full printout of test case 5 can be found in Appendix A.6.
The ﬁrst part of the printout can be seen in Figure 6.18. Step 1 is executed
and the soft kernel is turned oﬀon line 1. It is restarted on line 4 when step 2 is
performed. A warm start is used because the memory ﬁles are preserved from the
previous run.
1.
2.
3.
4.
5.
6.
7.
7.
8.
9.
10.
11.
12.
13.
14.
Figure 6.18. Result from test case 5:a
6.7. SUMMARY OF TEST RESULTS
53
7.
8.
9.
10.
11.
12.
13.
14.
14.
15.
16.
17.
18.
19.
20.
21.
Figure 6.19. Result from test case 5:b
The initiation of OSE is visible in Figure 6.19. During the initiation phase the
translation layer is started, see line 14, and the ﬂash scan function is initiated and
completed, see line 15 and 16 respectively.
When OSE is fully started, the print of the translation layer list in step 3 can be
carried out. The result can be seen in the ﬁgure of Appendix A.6. When compared
to the translation layer printout in Appendix A.5 it can be concluded that the scan
function is functional because the two printouts are identical.
6.7
Summary of test results
All tests have been carried out without error and the implemented translation layer
can therefore be considered a success.
Chapter 7
Discussion
The aim of this report was to design and optimize a ﬂash memory management
system for NAND-ﬂash and to build and implement a simpler design of a ﬂash
system in OSE.
7.1
Problem statement
This section will present the answers to the questions stated in Section 1.2.
7.1.1
Flash memory management design
What ﬂash management systems are available and is there a solution ﬁtting the
requirements?
There are many ﬂash management systems available and they can be split in two
groups; ﬂash dedicated ﬁle systems and ﬂash translation layers. This report takes a
closer look at the six ﬂash ﬁle systems JFFS, JFFS2, YAFFS, YAFFS2 CFFS and
MODA it also examines the ﬁve translation layers BAST, AFTL, FAST, FTL/FC
and MFTL.
The report concludes that the optimal ﬁle system is the CFFS scheme because of
its fast mount properties. The data clustering scheme in CFFS should, however, be
changed to the MODA variant for better performance. The wear leveling scheme
should also be combined with the one used in JFFS2 to be able to handle wear
caused by static data.
This report also concludes that an optimal ﬂash translation layer consist of
FAST FTL on the block-level mapped userdata in MFTL together with the JEFF
interface so that the translation layer can be aware of if incoming data is userdata
or metadata.
When choosing the overall optimal ﬂash management solution it is concluded
that a ﬂash translation layer together with JEFF is the best choice. This is con-
cluded although this requires journaling in both the ﬁle system and the translation
layer. The fact that JEFF is already a well working ﬁle system in OSE and that
55
56
CHAPTER 7. DISCUSSION
OSE does not always use ﬂash memory as a storage device are considered as stronger
arguments.
The optimized translation layer selected in Section 3.5 was the MFTL combined
with FAST FTL. This knowledge can, however, be combined with the knowledge
drawn from the Section 3.4.
The MODA scheme presented in Subsection 3.4.4
only separates data in the ﬁrst stage and will therefore not disrupt the internal
page locations during operation. It could therefore be a possibility to combine the
MODA scheme with the selected optimal translation layer. Metadata and userdata
separation is already included in MFTL so the diﬀerence with MODA would be the
allocation of userdata in hot and cold regions.
7.1.2
QoS and power awareness
How can power awareness and QoS be implemented on the ﬂash memory manage-
ment system?
The part of QoS included in the ﬂash memory management system is static
power management. The system is designed to be power eﬃcient and the algorithms
are chosen with the aims to reduce the writes and erases as suggested in 2.1.1. The
MODA clustering scheme will reduce copying overhead and thus reduce both ﬂash
reads and writes. That will also lead to less erases because the ﬂash memory is better
utilized. The FAST FTL is designed to minimize the number of erase operations
and is thus also a part of the static power management.
However, there are no obvious parts where dynamic power management can be
applied in this ﬂash management system. If no dynamic power management exists
there is no need for the system to be power-aware.
7.1.3
File system and FTL interface
How can a potential interface between a ﬁle systems and an optimal ﬂash memory
management system be improved?
The implementation of the MFTL scheme will work better if the interface be-
tween JEFF and the FTL uses the data classiﬁcation where metadata and userdata
can be distinguished. However, no modiﬁcation to the interface is necessary because
the separation between metadata and userdata is already included in the interface.
7.1.4
Flash memory management implementation
What will a suitable implementation design look like to be able to be constructed
within the limited time frame?
In order to build a functional FTL within the limited time frame a simple design
was established.
The design supports all necessary features of an FTL but no
optimization toward low power consumption and low RAM usage is applied.
The translation layers is run on a simulated NOR-ﬂash even though the design
is adapted for NAND-ﬂash. The reason for this is because there were no suitable
7.2. CONCLUSIONS
57
NAND-ﬂash developing tools available, the NOR-ﬂash interface has, however, been
altered to meet the more restrictive NAND-ﬂash requirements.
The designed FTL uses a logging approach when writing to the ﬂash volume
which means that all data is written sequentially. The garbage collection function
is adapted to the method of writing data and will therefore also clean blocks se-
quentially. During cleaning, which always takes place at the beginning of the log,
valid data is moved to the end of the log before the ﬁrst block in the log is erased.
The page mapping technique has been chosen for the mapping table due to its
simplicity.
Translation layer metadata is stored in two dedicated blocks in the beginning of
the memory. At reboot a scan function scans the metadata block and rebuilds the
mapping table.
The FTL uses the ddb.sig interface when communicating with JEFF and the
ﬂash_api.h interface is used when communicating with the FAM which stores the
data on the actual ﬂash memory.
7.1.5
Evaluation of implementation
Can the implemented design manage the basic requirements of a ﬂash memory man-
agement system?
The tests in the test suit are designed to determine whether the system can
manage the basic requirements of a translation layer.
All tests were carried out successfully and they showed that the implemented
system can handle read, write and erase operations together with normal ﬁle system
commands. It can also manage garbage collection and is able to recover data after
a reboot.
7.2
Conclusions
This section will compare the results from this master thesis with the requirements,
stated throughout the report and also collected in Appendix B, and conclude if they
are met.
Due to the fact that the optimized design from Section 3.7 diﬀers from the
implemented design in Chapter 5 the two will be compared separately.
7.2.1
Evaluation of requirements
REQ1: The design of the system shall be power eﬃcient.
The optimized design uses MFTL combined with FAST FTL and MODA and it
will therefore be a power eﬃcient solution. The prototype does, on the other hand,
use a design developed for simplicity and will therefore not reach the requirement
of being power eﬃcient.
REQ2: The system shall strive to be power-aware.
58
CHAPTER 7. DISCUSSION
It is a bit diﬃcult to determine if the results meet this requirement for the
optimized system. Eﬀorts have been made trying to ﬁnd ways to implement dynamic
power management in the ﬂash management system and thereby introduce the need
for power-awareness. Nevertheless, no dynamic power management situations have
been found. Still it is reasonable to conclude that a strive towards power-awareness
has taken place.
The implemented design has, however, no such aims and can therefore not be
concluded to have fulﬁlled the requirement.
REQ3: The number of erase and write operations shall be minimized.
The number of write and erase operations will be minimized by using the MFTL
with the FAST FTL scheme for the block level mapped partition and MODA on
the page level mapped partition, in the optimized design.
The implemented design, however, writes and erases data sequentially and thus
no minimizing of erase and write operation can ever occur.
REQ4: The system shall minimize RAM usage.
Selecting a block level approach for the userdata in MFTL in the optimized
design will have a major impact on the RAM footprint. Power eﬃciency is still
prioritized but the block level approach makes sure that the RAM-usage does not
go out of hand.
The implemented design does use a page-level mapping table and will therefore
consume a lot of RAM. It will therefore not reach the requirement for minimizing
RAM usage.
REQ5: The design shall focus on the NAND-ﬂash type.
Here both the optimized design and the implemented design meet the require-
ments. The optimized design is fully prepared for NAND-ﬂash and the implemented
design will work on NAND-ﬂash as soon as the function for storing metadata has
been altered.
REQ6: The system shall write pages in a block sequentially starting from the
ﬁrst page.
Met by both designs.
REQ7: A page shall only be written once before it is erased.
Met by both designs.
REQ8: The ﬂash memory management system shall use a translation layer
together with JEFF.
Both designs use a translation layer together with JEFF.
7.2.2
Summary of requirement evaluation
The evaluation of the requirements are summarized in Table 7.1. The optimized
design has met all requirements while the simpler implemented design failed to meet
the requirements concerning performance and power eﬃciency.
7.2. CONCLUSIONS
59
REQ no.
Optimized design
Implemented design
Description
REQ1
Pass
Fail
Power-eﬃciency
REQ2
Pass
Fail
Power-awareness
REQ3
Pass
Fail
Minimize writes and erases
REQ4
Pass
Fail
Minimize RAM usage
REQ5
Pass
Pass
NAND ﬂash requirements
REQ6
Pass
Pass
Write sequentially
REQ7
Pass
Pass
Write once requirement
REQ8
Pass
Pass
Translation layer + JEFF
Table 7.1. Evaluation of the requirements for the optimized and implemented de-
signs.
Chapter 8
Future work
This chapter will explain the work with the translation layer that still needs to be
done but could not be completed within the time restraints of this master thesis.
8.1
Build according to speciﬁcation
The most important part of the work that needs to be done in the future is to
upgrade the implemented translation layer to the system described in the speciﬁca-
tion. The implemented design completed in this master thesis is only ground work
for further development.
First of all the mapping table needs to be changed from page-mapped to block-
mapped. Consequently the linked list and the garbage collection mechanism needs
to be altered; the linked list holding the logical to physical page addresses needs
to be changed to blocks instead and the sequential garbage collection need to be
altered to meet the new demands concerning a block-mapped translation table, i.e.
be able to handle merge and shift operations.
When the system is constructed a performance evaluation needs to take place.
The questions that need answering are: How much energy can be saved with the
new system?, How big is the memory footprint and what is the throughput of the
system compared with the previous FTL?
8.2
Dynamic support for diﬀerent ﬂash sizes
The implemented system is not dynamic in the sense that ﬂash memories with
diﬀerent address ranges and block sizes can be used without manual modiﬁcation
to the code. As of now, the implemented system includes many hard coded values
in the code instead of making use of information extracted from the FAM.
Information such as block size and address range are extracted from the FAM
with the function get_ﬂash_characteristics during the initiation phase but that
information is not used in the main program. With a few modiﬁcations the trans-
61
62
CHAPTER 8. FUTURE WORK
lation layer could, however, be modiﬁed to be more dynamic and able to function
with diﬀerent ﬂash sizes without manual modiﬁcations
8.3
Flash translation layer metadata
The way of storing metadata in the implemented system is not suﬃcient and it is
not portable to a NAND ﬂash environment. The metadata is now stored in two
dedicated blocks at the beginning of the memory but in the future it would be better
to use the spare area available on NAND ﬂash.
The spare area in a ﬂash memory with a page size of 512 bytes will have a size
of 16 bytes but all bytes in the spare area cannot be used for just metadata. Space
also needs to be allocated for ECC and bad block management. In the previous
system used by ENEA involving the old FTL four bytes where used for metadata
so it should be safe to assume that so much space could be allocated for metadata
in the translation layer as well.
The physical address range goes from 0 - 0x3FFFFF which needs three bytes,
logic address range is 0 - 0x2000 which requires two bytes. This means that at least
5 bytes is necessary to represent all addresses. However, the system is supposed
to be designed for NAND ﬂash it is therefore not necessary to be able to address
every byte on the ﬂash memory. It is suﬃcient to just be able to address the start
byte of each page. this means that 0x3FFFFF could be divided by 512 or 0x200.
This leaves 0x1FFF pages which will ﬁt into two bytes. All necessary addresses can
therefore be represented by the 4 bytes available.
If a bigger ﬂash memory needs to be used a diﬀerent solution might be needed or
perhaps there actually are more bytes available in the spare area. It is important to
remember that NAND ﬂash memories with 512 byte ﬂash pages are the old model.
New bigger ﬂash memories has 2048 page size and a spare area of 64 byte which
should be enough to ﬁt the translation layer addresses.
8.4
Test system on hardware
So far the system has only been tested on a host computer with the soft kernel. To
make sure that system works properly it needs to be tested on proper hardware.
ENEA works with a Freescale board with an i.MX31 processor which can be use
for hardware testing. It has both NOR and NAND ﬂash available.
Bibliography
[1]
Unknown author.
View of /yaﬀs2/README-linux, Accessed:
October
16 2009.
URL http://www.aleph1.co.uk/cgi-bin/viewcvs.cgi/yaffs2/
README-linux?view=markup.
[2]
Seungjae Baek, Seongjun Ahn, Jongmoo Choi, Donghee Lee, and Sam H. Noh.
Uniformity Improving Page Allocation for Flash Memory File Systems. Pro-
ceedings of the 7th ACM and IEEE international conference on Embedded soft-
ware, pages 154 – 163, 2007.
[3]
Artem B. Bityutskiy.
JFFS3 design issues.
URL http://www.linux-mtd.
infradead.org/tech/JFFS3design.pdf. Version 0.32 (draft), 2005.
[4]
Mei-Ling Chiang, Chen-Lon Cheng, and Chun-Hung Wu. A New FTL-based
Flash Memory Management Scheme with Fast Cleaning Mechanism. The 2008
International Conference on Embedded Software and Systems, pages 205–214,
2008.
[5]
Mei-Ling Chiang, Paul C. H. Lee, and Ruei-Chuan Chang. Using Data Clus-
tering to Improve Cleaning Performance for Flash Memory. Software Practice
Experience, 29(3):267–290, March 1999. ISSN 0038-0644.
[6]
ENEA AB. Enea OSE Architecture User’s Guide, 2009.
[7]
ENEA AB. Enea OSE Core Extensions User Guide, November 2009.
[8]
ENEA AB. Enea OSE Core User’s Guide, October 2009.
[9]
Eran Gal and Sivan Toledo. Algorithms and Data Structures for Flash Mem-
ories. ACM Computing Surveys, 37(2):138–163, June 2005. ISSN 0360-0300.
[10] GEODES. GEODES (Global Energy Optimization for Distributed Embedded
Systems), Project number: ITEA2 <07013>, 2009. URL http://www.itea2.
org/public/project_leaflets/GEODES_profile_oct-08.pdf.
[11] Jeong-Uk Kang, Heeseung Jo, Jin-Soo Kim, and Joonwon Lee. A Superblock-
based Flash Translation Layer for NAND Flash Memory. Proceedings of the
6th ACM and IEEE International conference on Embedded software, pages 161
– 170, 2006.
63
64
BIBLIOGRAPHY
[12] Jesung Kim, Jong Min Kim, Sam H. Noh, Sang Lyul Min, and Yookun Cho.
A Space-Eﬃcient Flash Translation Layer for Compactﬂash Systems. IEEE
Transactions on Comsumer Electronics, 48(2):366–375, May 2002. ISSN 0098-
3063.
[13] San-Won Lee, Dong-Joo Park, Tae-Sun Chung, Dong-Ho Lee, Sangwon Park,
and Ha-Joo Song. A Log Buﬀer-Based Flash Translation LayerUsing Fully-
Associative Sector Translation. ACM Transactions on Embedded Computing
Systems, 6(3), July 2007.
[14] Han-Lin Li, Chia-Lin Yang, and Hung-Wei Tseng. Energy-Aware Flash Mem-
ory Management in Virtual Memory System.
IEEE Transactions on Very
Large Scale Integration (VLSI) Systems, 16(8):952–964, August 2008. ISSN
1063-8210.
[15] Charles Manning. How YAFFS Works. URL http://users.actrix.co.nz/
manningc/yaffs/HowYaffsWorks.pdf.
[16] Kyo-Ho Park and Seung-Ho Lim. An eﬃcient NAND Flash File System for
Flash Memory Storage. IEEE TRANSACTIONS ON COMPUTERS, 55(7):
906–912, July 2006. ISSN 0018-934.
[17] Padmanabhan Pillai, Hai Huang, and Kang G. Shin. Energy-Aware Quality of
Service Adaptation. The University of Michigan, 2003.
[18] David Woodhouse. JFFS: The Journalling Flash File System. URL http:
//sources/redhat/jffs2/jffs2.com. Ottawa Linux Symposium, 2001.
[19] Chin-Hsien Wu and Tei-Wei Kuo. An Adaptive Two-Level Management for
the Flash Translation Layer in Embedded Systems. Proceedings of the 2006
IEEE/ACM international conference on Computer-aided design, pages 601 –
606, 2006. ISSN 1092-3152.
[20] Po-Liang Wu, Yuan-Hao Chang, and Tei-Wei Kuo. A File-System-Aware FTL
Design for Flash-Memory Storage Systems. DATE09, 2009.
Appendix A
Complete test cases
A.1
Test case 1
1.
2.
3.
4.
5.
6.
7.
7.
8.
9.
10.
11.
12.
13.
14.
14.
15.
16.
17.
18.
19.
20.
21.
21.
22.
23.
Figure A.1. Print screen of the terminal showing the result of test case 1
65
66
APPENDIX A. COMPLETE TEST CASES
A.2
Test case 2
1.
2.
3.
4.
5.
6.
7.
7.
8.
9.
10.
11.
12.
13.
14.
14.
15.
16.
17.
18.
19.
20.
21.
21.
22.
23.
24.
25.
26.
27.
28.
28.
29.
30.
31.
32.
33.
34.
35.
35.
36.
37.
38.
39.
40.
41.
42.
42.
43.
44.
45.
46.
47.
48.
49.
49.
50.
51.
52.
53.
54.
55.
56.
56.
57.
58.
59.
60.
Figure A.2. Print screen of the terminal showing the result of test case 2
A.3. TEST CASE 3
67
A.3
Test case 3
1.
2.
3.
4.
5.
6.
7.
7.
8.
9.
10.
11.
12.
13.
14.
14.
15.
16.
17.
18.
19.
20.
21.
21.
21.
22.
23.
24.
25.
26.
27.
27.
28.
29.
30.
31.
32.
33.
34.
34.
35.
36.
37.
38.
39.
40.
41.
41.
42.
43.
44.
45.
46.
Figure A.3. Print screen of the terminal showing the result of test case 3
68
APPENDIX A. COMPLETE TEST CASES
A.4
Test case 4a
1.
2.
3.
4.
5.
6.
7.
7.
8.
9.
10.
11.
12.
13.
14.
14.
15.
16.
17.
18.
19.
20.
21.
21.
21.
22.
23.
24.
25.
26.
27.
27.
28.
29.
30.
31.
32.
33.
34.
34.
35.
36.
37.
38.
39.
40.
41.
41.
42.
43.
44.
45.
Figure A.4. Print screen of the terminal showing the result of test case 4a
A.5. TEST CASE 4B
69
A.5
Test case 4b
1.
2.
3.
4.
5.
6.
7.
7.
8.
9.
10.
11.
12.
13.
14.
14.
15.
16.
17.
18.
19.
20.
21.
21.
21.
22.
23.
24.
25.
26.
27.
27.
28.
29.
30.
31.
32.
33.
34.
34.
35.
36.
37.
38.
39.
40.
41.
41.
42.
43.
44.
45.
46.
47.
48.
48.
49.
50.
51.
52.
Figure A.5. Print screen of the terminal showing the result of test case 4b
70
APPENDIX A. COMPLETE TEST CASES
A.6
Test case 5
1.
2.
3.
4.
5.
6.
7.
7.
8.
9.
10.
11.
12.
13.
14.
14.
15.
16.
17.
18.
19.
20.
21.
21.
21.
22.
23.
24.
25.
26.
27.
27.
28.
29.
30.
31.
32.
33.
34.
34.
35.
36.
37.
38.
39.
40.
41.
41.
42.
43.
44.
45.
46.
47.
48.
48.
49.
50.
51.
52.
53.
54.
55.
55.
56.
57.
58.
59.
60.
61.
62.
62.
63.
64.
Figure A.6. Print screen of the terminal showing the result of test case 5
Appendix B
Requirements
ID
Description
REQ1
The design of the system shall be power eﬃcient.
REQ2
The system shall strive to be power aware.
REQ3
The number of erase and write operations shall be minimized.
REQ4
The system shall minimize the RAM-usage.
REQ5
The design of the system shall focus on the NAND-ﬂash memory type.
REQ6
The system shall write pages in a block sequentially starting from the
ﬁrst page.
REQ7
A page shall only be written once before it is erased.
REQ8
The ﬂash memory management system shall use a translation layer
together with JEFF.
Table B.1. Descriptions of the requirements.
71
